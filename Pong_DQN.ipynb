{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipendra7/2015lab1/blob/master/Pong_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DGro4Gt9zvo",
        "outputId": "0d80adf9-073f-4c29-ff13-8621417d758d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (6.4.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (4.66.5)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2024.8.30)\n",
            "Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=ca2b9cdca7d127d3ee542ff03751269da49c7c4fcefde7efd66efe7033e91252\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "test_env = gym.make(DEFAULT_ENV_NAME)\n",
        "print(test_env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsqdUcM1_iRC",
        "outputId": "7e862fdb-c308-4f3a-b4aa-72baa52e8207"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_env.unwrapped.get_action_meanings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie_3HVpg_o08",
        "outputId": "537eb1b8-0cad-42a7-c12c-ca1707461ee0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgUQYNV_qoK",
        "outputId": "85db7393-a893-4815-807c-c8def527aac4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ezGUJ_P_snz",
        "outputId": "7fcbbaf4-f31c-4208-9747-94b905ca6803"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VGyUSX3B_vL2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1],\n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "O8OUg1X9_xOQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn        # Pytorch neural network package\n",
        "import torch.optim as optim  # Pytorch optimization package\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "kV40hCBn_zvK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "wRYdCmum_1nQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = make_env(DEFAULT_ENV_NAME)\n",
        "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
        "print(test_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upZJ0LAN_4Zn",
        "outputId": "b0d9b80d-1b84-46ac-d03d-5426a1957278"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "48l3bY8SAQy7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "MEAN_REWARD_BOUND = 19.0\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 32\n",
        "replay_size = 10000\n",
        "learning_rate = 1e-4\n",
        "sync_target_frames = 1000\n",
        "replay_start_size = 10000\n",
        "\n",
        "eps_start=1.0\n",
        "eps_decay=.999985\n",
        "eps_min=0.02"
      ],
      "metadata": {
        "id": "BIcESgTJATVr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "metadata": {
        "id": "wxiRAdYLAVx9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "KIwwK64qAYHU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cItdcabnAa_d",
        "outputId": "e7e6994c-cfdd-46e5-a3fa-3937e7f4e526"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>Training starts at  2024-09-09 19:08:06.368770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
        "\n",
        "buffer = ExperienceReplay(replay_size)\n",
        "agent = Agent(env, buffer)\n",
        "\n",
        "epsilon = eps_start\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "\n",
        "best_mean_reward = None\n",
        "\n",
        "while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon*eps_decay, eps_min)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
        "\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        states_v = torch.tensor(states).to(device)\n",
        "        next_states_v = torch.tensor(next_states).to(device)\n",
        "        actions_v = torch.tensor(actions).to(device)\n",
        "        rewards_v = torch.tensor(rewards).to(device)\n",
        "        #done_mask = torch.ByteTensor(dones).to(device)\n",
        "        done_mask = torch.BoolTensor(dones).to(device) # Change to BoolTensor instead of ByteTensor\n",
        "\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        next_state_values[done_mask] = 0.0\n",
        "\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % sync_target_frames == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZfaVLJrAc9v",
        "outputId": "b43b0ef0-ad84-4e41-b6bd-93dce4113370"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "841:  1 games, mean reward -20.000, (epsilon 0.99)\n",
            "Best mean reward updated -20.000\n",
            "1927:  2 games, mean reward -19.000, (epsilon 0.97)\n",
            "Best mean reward updated -19.000\n",
            "2986:  3 games, mean reward -19.333, (epsilon 0.96)\n",
            "3809:  4 games, mean reward -19.750, (epsilon 0.94)\n",
            "4634:  5 games, mean reward -20.000, (epsilon 0.93)\n",
            "5642:  6 games, mean reward -20.000, (epsilon 0.92)\n",
            "6561:  7 games, mean reward -20.000, (epsilon 0.91)\n",
            "7401:  8 games, mean reward -20.000, (epsilon 0.89)\n",
            "8224:  9 games, mean reward -20.111, (epsilon 0.88)\n",
            "9167:  10 games, mean reward -20.100, (epsilon 0.87)\n",
            "9992:  11 games, mean reward -20.182, (epsilon 0.86)\n",
            "10962:  12 games, mean reward -20.167, (epsilon 0.85)\n",
            "11771:  13 games, mean reward -20.231, (epsilon 0.84)\n",
            "12733:  14 games, mean reward -20.214, (epsilon 0.83)\n",
            "13687:  15 games, mean reward -20.200, (epsilon 0.81)\n",
            "14640:  16 games, mean reward -20.125, (epsilon 0.80)\n",
            "15738:  17 games, mean reward -20.118, (epsilon 0.79)\n",
            "16579:  18 games, mean reward -20.111, (epsilon 0.78)\n",
            "17591:  19 games, mean reward -20.053, (epsilon 0.77)\n",
            "18354:  20 games, mean reward -20.100, (epsilon 0.76)\n",
            "19153:  21 games, mean reward -20.143, (epsilon 0.75)\n",
            "19972:  22 games, mean reward -20.182, (epsilon 0.74)\n",
            "20964:  23 games, mean reward -20.130, (epsilon 0.73)\n",
            "21894:  24 games, mean reward -20.167, (epsilon 0.72)\n",
            "22657:  25 games, mean reward -20.200, (epsilon 0.71)\n",
            "23420:  26 games, mean reward -20.231, (epsilon 0.70)\n",
            "24466:  27 games, mean reward -20.259, (epsilon 0.69)\n",
            "25317:  28 games, mean reward -20.286, (epsilon 0.68)\n",
            "26185:  29 games, mean reward -20.276, (epsilon 0.68)\n",
            "26994:  30 games, mean reward -20.300, (epsilon 0.67)\n",
            "27757:  31 games, mean reward -20.323, (epsilon 0.66)\n",
            "28641:  32 games, mean reward -20.344, (epsilon 0.65)\n",
            "29452:  33 games, mean reward -20.364, (epsilon 0.64)\n",
            "30464:  34 games, mean reward -20.382, (epsilon 0.63)\n",
            "31420:  35 games, mean reward -20.400, (epsilon 0.62)\n",
            "32645:  36 games, mean reward -20.361, (epsilon 0.61)\n",
            "33620:  37 games, mean reward -20.351, (epsilon 0.60)\n",
            "34491:  38 games, mean reward -20.368, (epsilon 0.60)\n",
            "35392:  39 games, mean reward -20.359, (epsilon 0.59)\n",
            "36243:  40 games, mean reward -20.375, (epsilon 0.58)\n",
            "37420:  41 games, mean reward -20.317, (epsilon 0.57)\n",
            "38406:  42 games, mean reward -20.333, (epsilon 0.56)\n",
            "39569:  43 games, mean reward -20.279, (epsilon 0.55)\n",
            "40700:  44 games, mean reward -20.205, (epsilon 0.54)\n",
            "42168:  45 games, mean reward -20.133, (epsilon 0.53)\n",
            "43472:  46 games, mean reward -20.087, (epsilon 0.52)\n",
            "44490:  47 games, mean reward -20.085, (epsilon 0.51)\n",
            "45754:  48 games, mean reward -20.062, (epsilon 0.50)\n",
            "47239:  49 games, mean reward -20.020, (epsilon 0.49)\n",
            "48438:  50 games, mean reward -20.040, (epsilon 0.48)\n",
            "49750:  51 games, mean reward -20.000, (epsilon 0.47)\n",
            "50878:  52 games, mean reward -19.981, (epsilon 0.47)\n",
            "52227:  53 games, mean reward -19.943, (epsilon 0.46)\n",
            "53519:  54 games, mean reward -19.963, (epsilon 0.45)\n",
            "54708:  55 games, mean reward -19.982, (epsilon 0.44)\n",
            "56006:  56 games, mean reward -19.982, (epsilon 0.43)\n",
            "57450:  57 games, mean reward -20.000, (epsilon 0.42)\n",
            "59002:  58 games, mean reward -19.948, (epsilon 0.41)\n",
            "60218:  59 games, mean reward -19.932, (epsilon 0.41)\n",
            "61615:  60 games, mean reward -19.917, (epsilon 0.40)\n",
            "63241:  61 games, mean reward -19.885, (epsilon 0.39)\n",
            "64648:  62 games, mean reward -19.855, (epsilon 0.38)\n",
            "65929:  63 games, mean reward -19.841, (epsilon 0.37)\n",
            "67822:  64 games, mean reward -19.812, (epsilon 0.36)\n",
            "69366:  65 games, mean reward -19.785, (epsilon 0.35)\n",
            "70491:  66 games, mean reward -19.788, (epsilon 0.35)\n",
            "71998:  67 games, mean reward -19.761, (epsilon 0.34)\n",
            "73230:  68 games, mean reward -19.765, (epsilon 0.33)\n",
            "74678:  69 games, mean reward -19.783, (epsilon 0.33)\n",
            "76205:  70 games, mean reward -19.786, (epsilon 0.32)\n",
            "77767:  71 games, mean reward -19.761, (epsilon 0.31)\n",
            "79466:  72 games, mean reward -19.736, (epsilon 0.30)\n",
            "81089:  73 games, mean reward -19.740, (epsilon 0.30)\n",
            "82908:  74 games, mean reward -19.703, (epsilon 0.29)\n",
            "84910:  75 games, mean reward -19.640, (epsilon 0.28)\n",
            "86001:  76 games, mean reward -19.658, (epsilon 0.28)\n",
            "88128:  77 games, mean reward -19.623, (epsilon 0.27)\n",
            "89641:  78 games, mean reward -19.628, (epsilon 0.26)\n",
            "92227:  79 games, mean reward -19.519, (epsilon 0.25)\n",
            "94563:  80 games, mean reward -19.500, (epsilon 0.24)\n",
            "95971:  81 games, mean reward -19.506, (epsilon 0.24)\n",
            "97518:  82 games, mean reward -19.488, (epsilon 0.23)\n",
            "99412:  83 games, mean reward -19.434, (epsilon 0.23)\n",
            "101472:  84 games, mean reward -19.405, (epsilon 0.22)\n",
            "103592:  85 games, mean reward -19.365, (epsilon 0.21)\n",
            "105499:  86 games, mean reward -19.337, (epsilon 0.21)\n",
            "107094:  87 games, mean reward -19.333, (epsilon 0.20)\n",
            "109573:  88 games, mean reward -19.330, (epsilon 0.19)\n",
            "111930:  89 games, mean reward -19.247, (epsilon 0.19)\n",
            "113702:  90 games, mean reward -19.222, (epsilon 0.18)\n",
            "115859:  91 games, mean reward -19.154, (epsilon 0.18)\n",
            "117956:  92 games, mean reward -19.120, (epsilon 0.17)\n",
            "119975:  93 games, mean reward -19.065, (epsilon 0.17)\n",
            "122338:  94 games, mean reward -19.021, (epsilon 0.16)\n",
            "124942:  95 games, mean reward -18.947, (epsilon 0.15)\n",
            "Best mean reward updated -18.947\n",
            "127147:  96 games, mean reward -18.917, (epsilon 0.15)\n",
            "Best mean reward updated -18.917\n",
            "130150:  97 games, mean reward -18.845, (epsilon 0.14)\n",
            "Best mean reward updated -18.845\n",
            "132410:  98 games, mean reward -18.806, (epsilon 0.14)\n",
            "Best mean reward updated -18.806\n",
            "134316:  99 games, mean reward -18.798, (epsilon 0.13)\n",
            "Best mean reward updated -18.798\n",
            "136299:  100 games, mean reward -18.780, (epsilon 0.13)\n",
            "Best mean reward updated -18.780\n",
            "139011:  101 games, mean reward -18.710, (epsilon 0.12)\n",
            "Best mean reward updated -18.710\n",
            "142175:  102 games, mean reward -18.640, (epsilon 0.12)\n",
            "Best mean reward updated -18.640\n",
            "144344:  103 games, mean reward -18.620, (epsilon 0.11)\n",
            "Best mean reward updated -18.620\n",
            "147017:  104 games, mean reward -18.570, (epsilon 0.11)\n",
            "Best mean reward updated -18.570\n",
            "149192:  105 games, mean reward -18.520, (epsilon 0.11)\n",
            "Best mean reward updated -18.520\n",
            "151494:  106 games, mean reward -18.460, (epsilon 0.10)\n",
            "Best mean reward updated -18.460\n",
            "154383:  107 games, mean reward -18.390, (epsilon 0.10)\n",
            "Best mean reward updated -18.390\n",
            "156734:  108 games, mean reward -18.380, (epsilon 0.10)\n",
            "Best mean reward updated -18.380\n",
            "158696:  109 games, mean reward -18.350, (epsilon 0.09)\n",
            "Best mean reward updated -18.350\n",
            "160957:  110 games, mean reward -18.300, (epsilon 0.09)\n",
            "Best mean reward updated -18.300\n",
            "163466:  111 games, mean reward -18.250, (epsilon 0.09)\n",
            "Best mean reward updated -18.250\n",
            "166750:  112 games, mean reward -18.140, (epsilon 0.08)\n",
            "Best mean reward updated -18.140\n",
            "169461:  113 games, mean reward -18.010, (epsilon 0.08)\n",
            "Best mean reward updated -18.010\n",
            "172378:  114 games, mean reward -17.870, (epsilon 0.08)\n",
            "Best mean reward updated -17.870\n",
            "174790:  115 games, mean reward -17.820, (epsilon 0.07)\n",
            "Best mean reward updated -17.820\n",
            "178323:  116 games, mean reward -17.650, (epsilon 0.07)\n",
            "Best mean reward updated -17.650\n",
            "182087:  117 games, mean reward -17.500, (epsilon 0.07)\n",
            "Best mean reward updated -17.500\n",
            "185113:  118 games, mean reward -17.390, (epsilon 0.06)\n",
            "Best mean reward updated -17.390\n",
            "188845:  119 games, mean reward -17.180, (epsilon 0.06)\n",
            "Best mean reward updated -17.180\n",
            "191920:  120 games, mean reward -16.950, (epsilon 0.06)\n",
            "Best mean reward updated -16.950\n",
            "195667:  121 games, mean reward -16.750, (epsilon 0.05)\n",
            "Best mean reward updated -16.750\n",
            "198458:  122 games, mean reward -16.450, (epsilon 0.05)\n",
            "Best mean reward updated -16.450\n",
            "201473:  123 games, mean reward -16.220, (epsilon 0.05)\n",
            "Best mean reward updated -16.220\n",
            "203899:  124 games, mean reward -15.900, (epsilon 0.05)\n",
            "Best mean reward updated -15.900\n",
            "206063:  125 games, mean reward -15.540, (epsilon 0.05)\n",
            "Best mean reward updated -15.540\n",
            "208594:  126 games, mean reward -15.230, (epsilon 0.04)\n",
            "Best mean reward updated -15.230\n",
            "211210:  127 games, mean reward -14.950, (epsilon 0.04)\n",
            "Best mean reward updated -14.950\n",
            "213761:  128 games, mean reward -14.620, (epsilon 0.04)\n",
            "Best mean reward updated -14.620\n",
            "217025:  129 games, mean reward -14.460, (epsilon 0.04)\n",
            "Best mean reward updated -14.460\n",
            "220072:  130 games, mean reward -14.150, (epsilon 0.04)\n",
            "Best mean reward updated -14.150\n",
            "222175:  131 games, mean reward -13.800, (epsilon 0.04)\n",
            "Best mean reward updated -13.800\n",
            "224137:  132 games, mean reward -13.420, (epsilon 0.03)\n",
            "Best mean reward updated -13.420\n",
            "226494:  133 games, mean reward -13.090, (epsilon 0.03)\n",
            "Best mean reward updated -13.090\n",
            "228250:  134 games, mean reward -12.680, (epsilon 0.03)\n",
            "Best mean reward updated -12.680\n",
            "230376:  135 games, mean reward -12.310, (epsilon 0.03)\n",
            "Best mean reward updated -12.310\n",
            "232484:  136 games, mean reward -11.970, (epsilon 0.03)\n",
            "Best mean reward updated -11.970\n",
            "234881:  137 games, mean reward -11.700, (epsilon 0.03)\n",
            "Best mean reward updated -11.700\n",
            "237629:  138 games, mean reward -11.410, (epsilon 0.03)\n",
            "Best mean reward updated -11.410\n",
            "241232:  139 games, mean reward -11.170, (epsilon 0.03)\n",
            "Best mean reward updated -11.170\n",
            "243768:  140 games, mean reward -10.820, (epsilon 0.03)\n",
            "Best mean reward updated -10.820\n",
            "245902:  141 games, mean reward -10.490, (epsilon 0.03)\n",
            "Best mean reward updated -10.490\n",
            "247586:  142 games, mean reward -10.080, (epsilon 0.02)\n",
            "Best mean reward updated -10.080\n",
            "249721:  143 games, mean reward -9.730, (epsilon 0.02)\n",
            "Best mean reward updated -9.730\n",
            "251618:  144 games, mean reward -9.390, (epsilon 0.02)\n",
            "Best mean reward updated -9.390\n",
            "253815:  145 games, mean reward -9.070, (epsilon 0.02)\n",
            "Best mean reward updated -9.070\n",
            "257180:  146 games, mean reward -8.820, (epsilon 0.02)\n",
            "Best mean reward updated -8.820\n",
            "259147:  147 games, mean reward -8.450, (epsilon 0.02)\n",
            "Best mean reward updated -8.450\n",
            "261421:  148 games, mean reward -8.120, (epsilon 0.02)\n",
            "Best mean reward updated -8.120\n",
            "263114:  149 games, mean reward -7.740, (epsilon 0.02)\n",
            "Best mean reward updated -7.740\n",
            "265569:  150 games, mean reward -7.410, (epsilon 0.02)\n",
            "Best mean reward updated -7.410\n",
            "267486:  151 games, mean reward -7.060, (epsilon 0.02)\n",
            "Best mean reward updated -7.060\n",
            "269419:  152 games, mean reward -6.680, (epsilon 0.02)\n",
            "Best mean reward updated -6.680\n",
            "271324:  153 games, mean reward -6.330, (epsilon 0.02)\n",
            "Best mean reward updated -6.330\n",
            "273719:  154 games, mean reward -5.980, (epsilon 0.02)\n",
            "Best mean reward updated -5.980\n",
            "276417:  155 games, mean reward -5.700, (epsilon 0.02)\n",
            "Best mean reward updated -5.700\n",
            "278846:  156 games, mean reward -5.370, (epsilon 0.02)\n",
            "Best mean reward updated -5.370\n",
            "280565:  157 games, mean reward -4.960, (epsilon 0.02)\n",
            "Best mean reward updated -4.960\n",
            "283191:  158 games, mean reward -4.660, (epsilon 0.02)\n",
            "Best mean reward updated -4.660\n",
            "285150:  159 games, mean reward -4.300, (epsilon 0.02)\n",
            "Best mean reward updated -4.300\n",
            "287405:  160 games, mean reward -3.960, (epsilon 0.02)\n",
            "Best mean reward updated -3.960\n",
            "289474:  161 games, mean reward -3.610, (epsilon 0.02)\n",
            "Best mean reward updated -3.610\n",
            "291287:  162 games, mean reward -3.220, (epsilon 0.02)\n",
            "Best mean reward updated -3.220\n",
            "293480:  163 games, mean reward -2.850, (epsilon 0.02)\n",
            "Best mean reward updated -2.850\n",
            "295261:  164 games, mean reward -2.480, (epsilon 0.02)\n",
            "Best mean reward updated -2.480\n",
            "297010:  165 games, mean reward -2.100, (epsilon 0.02)\n",
            "Best mean reward updated -2.100\n",
            "298938:  166 games, mean reward -1.720, (epsilon 0.02)\n",
            "Best mean reward updated -1.720\n",
            "301640:  167 games, mean reward -1.460, (epsilon 0.02)\n",
            "Best mean reward updated -1.460\n",
            "303704:  168 games, mean reward -1.110, (epsilon 0.02)\n",
            "Best mean reward updated -1.110\n",
            "305427:  169 games, mean reward -0.690, (epsilon 0.02)\n",
            "Best mean reward updated -0.690\n",
            "307289:  170 games, mean reward -0.290, (epsilon 0.02)\n",
            "Best mean reward updated -0.290\n",
            "308919:  171 games, mean reward 0.100, (epsilon 0.02)\n",
            "Best mean reward updated 0.100\n",
            "310723:  172 games, mean reward 0.470, (epsilon 0.02)\n",
            "Best mean reward updated 0.470\n",
            "313430:  173 games, mean reward 0.820, (epsilon 0.02)\n",
            "Best mean reward updated 0.820\n",
            "315577:  174 games, mean reward 1.150, (epsilon 0.02)\n",
            "Best mean reward updated 1.150\n",
            "317434:  175 games, mean reward 1.480, (epsilon 0.02)\n",
            "Best mean reward updated 1.480\n",
            "319432:  176 games, mean reward 1.880, (epsilon 0.02)\n",
            "Best mean reward updated 1.880\n",
            "321209:  177 games, mean reward 2.240, (epsilon 0.02)\n",
            "Best mean reward updated 2.240\n",
            "322950:  178 games, mean reward 2.640, (epsilon 0.02)\n",
            "Best mean reward updated 2.640\n",
            "324774:  179 games, mean reward 2.940, (epsilon 0.02)\n",
            "Best mean reward updated 2.940\n",
            "326724:  180 games, mean reward 3.300, (epsilon 0.02)\n",
            "Best mean reward updated 3.300\n",
            "328472:  181 games, mean reward 3.700, (epsilon 0.02)\n",
            "Best mean reward updated 3.700\n",
            "330164:  182 games, mean reward 4.090, (epsilon 0.02)\n",
            "Best mean reward updated 4.090\n",
            "332239:  183 games, mean reward 4.400, (epsilon 0.02)\n",
            "Best mean reward updated 4.400\n",
            "334288:  184 games, mean reward 4.750, (epsilon 0.02)\n",
            "Best mean reward updated 4.750\n",
            "336118:  185 games, mean reward 5.080, (epsilon 0.02)\n",
            "Best mean reward updated 5.080\n",
            "338248:  186 games, mean reward 5.410, (epsilon 0.02)\n",
            "Best mean reward updated 5.410\n",
            "340407:  187 games, mean reward 5.700, (epsilon 0.02)\n",
            "Best mean reward updated 5.700\n",
            "342749:  188 games, mean reward 6.030, (epsilon 0.02)\n",
            "Best mean reward updated 6.030\n",
            "344501:  189 games, mean reward 6.360, (epsilon 0.02)\n",
            "Best mean reward updated 6.360\n",
            "346951:  190 games, mean reward 6.660, (epsilon 0.02)\n",
            "Best mean reward updated 6.660\n",
            "349545:  191 games, mean reward 6.940, (epsilon 0.02)\n",
            "Best mean reward updated 6.940\n",
            "351458:  192 games, mean reward 7.290, (epsilon 0.02)\n",
            "Best mean reward updated 7.290\n",
            "353280:  193 games, mean reward 7.620, (epsilon 0.02)\n",
            "Best mean reward updated 7.620\n",
            "355120:  194 games, mean reward 7.960, (epsilon 0.02)\n",
            "Best mean reward updated 7.960\n",
            "357139:  195 games, mean reward 8.270, (epsilon 0.02)\n",
            "Best mean reward updated 8.270\n",
            "359157:  196 games, mean reward 8.590, (epsilon 0.02)\n",
            "Best mean reward updated 8.590\n",
            "360973:  197 games, mean reward 8.910, (epsilon 0.02)\n",
            "Best mean reward updated 8.910\n",
            "362755:  198 games, mean reward 9.260, (epsilon 0.02)\n",
            "Best mean reward updated 9.260\n",
            "364435:  199 games, mean reward 9.640, (epsilon 0.02)\n",
            "Best mean reward updated 9.640\n",
            "366165:  200 games, mean reward 10.010, (epsilon 0.02)\n",
            "Best mean reward updated 10.010\n",
            "367993:  201 games, mean reward 10.330, (epsilon 0.02)\n",
            "Best mean reward updated 10.330\n",
            "370133:  202 games, mean reward 10.600, (epsilon 0.02)\n",
            "Best mean reward updated 10.600\n",
            "371763:  203 games, mean reward 10.990, (epsilon 0.02)\n",
            "Best mean reward updated 10.990\n",
            "373768:  204 games, mean reward 11.300, (epsilon 0.02)\n",
            "Best mean reward updated 11.300\n",
            "375398:  205 games, mean reward 11.670, (epsilon 0.02)\n",
            "Best mean reward updated 11.670\n",
            "377985:  206 games, mean reward 11.890, (epsilon 0.02)\n",
            "Best mean reward updated 11.890\n",
            "379615:  207 games, mean reward 12.230, (epsilon 0.02)\n",
            "Best mean reward updated 12.230\n",
            "381446:  208 games, mean reward 12.600, (epsilon 0.02)\n",
            "Best mean reward updated 12.600\n",
            "383136:  209 games, mean reward 12.980, (epsilon 0.02)\n",
            "Best mean reward updated 12.980\n",
            "384798:  210 games, mean reward 13.330, (epsilon 0.02)\n",
            "Best mean reward updated 13.330\n",
            "386729:  211 games, mean reward 13.670, (epsilon 0.02)\n",
            "Best mean reward updated 13.670\n",
            "388611:  212 games, mean reward 13.940, (epsilon 0.02)\n",
            "Best mean reward updated 13.940\n",
            "390276:  213 games, mean reward 14.220, (epsilon 0.02)\n",
            "Best mean reward updated 14.220\n",
            "392072:  214 games, mean reward 14.460, (epsilon 0.02)\n",
            "Best mean reward updated 14.460\n",
            "393863:  215 games, mean reward 14.790, (epsilon 0.02)\n",
            "Best mean reward updated 14.790\n",
            "395831:  216 games, mean reward 14.970, (epsilon 0.02)\n",
            "Best mean reward updated 14.970\n",
            "397985:  217 games, mean reward 15.170, (epsilon 0.02)\n",
            "Best mean reward updated 15.170\n",
            "400106:  218 games, mean reward 15.420, (epsilon 0.02)\n",
            "Best mean reward updated 15.420\n",
            "401954:  219 games, mean reward 15.580, (epsilon 0.02)\n",
            "Best mean reward updated 15.580\n",
            "403796:  220 games, mean reward 15.740, (epsilon 0.02)\n",
            "Best mean reward updated 15.740\n",
            "405518:  221 games, mean reward 15.950, (epsilon 0.02)\n",
            "Best mean reward updated 15.950\n",
            "407333:  222 games, mean reward 16.060, (epsilon 0.02)\n",
            "Best mean reward updated 16.060\n",
            "409221:  223 games, mean reward 16.200, (epsilon 0.02)\n",
            "Best mean reward updated 16.200\n",
            "411232:  224 games, mean reward 16.220, (epsilon 0.02)\n",
            "Best mean reward updated 16.220\n",
            "412923:  225 games, mean reward 16.270, (epsilon 0.02)\n",
            "Best mean reward updated 16.270\n",
            "414939:  226 games, mean reward 16.300, (epsilon 0.02)\n",
            "Best mean reward updated 16.300\n",
            "416738:  227 games, mean reward 16.420, (epsilon 0.02)\n",
            "Best mean reward updated 16.420\n",
            "418555:  228 games, mean reward 16.490, (epsilon 0.02)\n",
            "Best mean reward updated 16.490\n",
            "420384:  229 games, mean reward 16.720, (epsilon 0.02)\n",
            "Best mean reward updated 16.720\n",
            "422452:  230 games, mean reward 16.810, (epsilon 0.02)\n",
            "Best mean reward updated 16.810\n",
            "424370:  231 games, mean reward 16.850, (epsilon 0.02)\n",
            "Best mean reward updated 16.850\n",
            "426184:  232 games, mean reward 16.880, (epsilon 0.02)\n",
            "Best mean reward updated 16.880\n",
            "427814:  233 games, mean reward 16.970, (epsilon 0.02)\n",
            "Best mean reward updated 16.970\n",
            "429572:  234 games, mean reward 16.960, (epsilon 0.02)\n",
            "431233:  235 games, mean reward 17.010, (epsilon 0.02)\n",
            "Best mean reward updated 17.010\n",
            "432863:  236 games, mean reward 17.070, (epsilon 0.02)\n",
            "Best mean reward updated 17.070\n",
            "434723:  237 games, mean reward 17.190, (epsilon 0.02)\n",
            "Best mean reward updated 17.190\n",
            "436372:  238 games, mean reward 17.320, (epsilon 0.02)\n",
            "Best mean reward updated 17.320\n",
            "438099:  239 games, mean reward 17.480, (epsilon 0.02)\n",
            "Best mean reward updated 17.480\n",
            "440246:  240 games, mean reward 17.490, (epsilon 0.02)\n",
            "Best mean reward updated 17.490\n",
            "442539:  241 games, mean reward 17.500, (epsilon 0.02)\n",
            "Best mean reward updated 17.500\n",
            "444283:  242 games, mean reward 17.500, (epsilon 0.02)\n",
            "446208:  243 games, mean reward 17.530, (epsilon 0.02)\n",
            "Best mean reward updated 17.530\n",
            "448088:  244 games, mean reward 17.550, (epsilon 0.02)\n",
            "Best mean reward updated 17.550\n",
            "450000:  245 games, mean reward 17.590, (epsilon 0.02)\n",
            "Best mean reward updated 17.590\n",
            "451632:  246 games, mean reward 17.730, (epsilon 0.02)\n",
            "Best mean reward updated 17.730\n",
            "453547:  247 games, mean reward 17.740, (epsilon 0.02)\n",
            "Best mean reward updated 17.740\n",
            "455181:  248 games, mean reward 17.810, (epsilon 0.02)\n",
            "Best mean reward updated 17.810\n",
            "457522:  249 games, mean reward 17.730, (epsilon 0.02)\n",
            "459743:  250 games, mean reward 17.770, (epsilon 0.02)\n",
            "461867:  251 games, mean reward 17.770, (epsilon 0.02)\n",
            "463529:  252 games, mean reward 17.780, (epsilon 0.02)\n",
            "465599:  253 games, mean reward 17.760, (epsilon 0.02)\n",
            "467383:  254 games, mean reward 17.820, (epsilon 0.02)\n",
            "Best mean reward updated 17.820\n",
            "469068:  255 games, mean reward 17.950, (epsilon 0.02)\n",
            "Best mean reward updated 17.950\n",
            "470826:  256 games, mean reward 18.020, (epsilon 0.02)\n",
            "Best mean reward updated 18.020\n",
            "472581:  257 games, mean reward 18.020, (epsilon 0.02)\n",
            "474511:  258 games, mean reward 18.060, (epsilon 0.02)\n",
            "Best mean reward updated 18.060\n",
            "476397:  259 games, mean reward 18.040, (epsilon 0.02)\n",
            "478387:  260 games, mean reward 18.020, (epsilon 0.02)\n",
            "480129:  261 games, mean reward 18.050, (epsilon 0.02)\n",
            "482014:  262 games, mean reward 18.040, (epsilon 0.02)\n",
            "483760:  263 games, mean reward 18.060, (epsilon 0.02)\n",
            "485581:  264 games, mean reward 18.060, (epsilon 0.02)\n",
            "487213:  265 games, mean reward 18.070, (epsilon 0.02)\n",
            "Best mean reward updated 18.070\n",
            "489064:  266 games, mean reward 18.050, (epsilon 0.02)\n",
            "490988:  267 games, mean reward 18.140, (epsilon 0.02)\n",
            "Best mean reward updated 18.140\n",
            "492903:  268 games, mean reward 18.170, (epsilon 0.02)\n",
            "Best mean reward updated 18.170\n",
            "494739:  269 games, mean reward 18.140, (epsilon 0.02)\n",
            "496405:  270 games, mean reward 18.150, (epsilon 0.02)\n",
            "498182:  271 games, mean reward 18.130, (epsilon 0.02)\n",
            "500266:  272 games, mean reward 18.110, (epsilon 0.02)\n",
            "502324:  273 games, mean reward 18.130, (epsilon 0.02)\n",
            "503958:  274 games, mean reward 18.180, (epsilon 0.02)\n",
            "Best mean reward updated 18.180\n",
            "505752:  275 games, mean reward 18.190, (epsilon 0.02)\n",
            "Best mean reward updated 18.190\n",
            "507481:  276 games, mean reward 18.200, (epsilon 0.02)\n",
            "Best mean reward updated 18.200\n",
            "509113:  277 games, mean reward 18.220, (epsilon 0.02)\n",
            "Best mean reward updated 18.220\n",
            "511337:  278 games, mean reward 18.180, (epsilon 0.02)\n",
            "512975:  279 games, mean reward 18.200, (epsilon 0.02)\n",
            "514939:  280 games, mean reward 18.170, (epsilon 0.02)\n",
            "516916:  281 games, mean reward 18.140, (epsilon 0.02)\n",
            "518737:  282 games, mean reward 18.130, (epsilon 0.02)\n",
            "520960:  283 games, mean reward 18.110, (epsilon 0.02)\n",
            "522683:  284 games, mean reward 18.130, (epsilon 0.02)\n",
            "524997:  285 games, mean reward 18.070, (epsilon 0.02)\n",
            "527059:  286 games, mean reward 18.090, (epsilon 0.02)\n",
            "528689:  287 games, mean reward 18.200, (epsilon 0.02)\n",
            "530323:  288 games, mean reward 18.270, (epsilon 0.02)\n",
            "Best mean reward updated 18.270\n",
            "532153:  289 games, mean reward 18.260, (epsilon 0.02)\n",
            "534365:  290 games, mean reward 18.280, (epsilon 0.02)\n",
            "Best mean reward updated 18.280\n",
            "536385:  291 games, mean reward 18.290, (epsilon 0.02)\n",
            "Best mean reward updated 18.290\n",
            "538497:  292 games, mean reward 18.250, (epsilon 0.02)\n",
            "540131:  293 games, mean reward 18.270, (epsilon 0.02)\n",
            "541852:  294 games, mean reward 18.280, (epsilon 0.02)\n",
            "543543:  295 games, mean reward 18.290, (epsilon 0.02)\n",
            "545209:  296 games, mean reward 18.330, (epsilon 0.02)\n",
            "Best mean reward updated 18.330\n",
            "547047:  297 games, mean reward 18.320, (epsilon 0.02)\n",
            "549129:  298 games, mean reward 18.290, (epsilon 0.02)\n",
            "550974:  299 games, mean reward 18.280, (epsilon 0.02)\n",
            "552762:  300 games, mean reward 18.280, (epsilon 0.02)\n",
            "554857:  301 games, mean reward 18.270, (epsilon 0.02)\n",
            "556749:  302 games, mean reward 18.300, (epsilon 0.02)\n",
            "558381:  303 games, mean reward 18.300, (epsilon 0.02)\n",
            "560157:  304 games, mean reward 18.350, (epsilon 0.02)\n",
            "Best mean reward updated 18.350\n",
            "562096:  305 games, mean reward 18.310, (epsilon 0.02)\n",
            "563726:  306 games, mean reward 18.440, (epsilon 0.02)\n",
            "Best mean reward updated 18.440\n",
            "565456:  307 games, mean reward 18.430, (epsilon 0.02)\n",
            "567632:  308 games, mean reward 18.390, (epsilon 0.02)\n",
            "569481:  309 games, mean reward 18.380, (epsilon 0.02)\n",
            "571179:  310 games, mean reward 18.370, (epsilon 0.02)\n",
            "572814:  311 games, mean reward 18.400, (epsilon 0.02)\n",
            "575101:  312 games, mean reward 18.310, (epsilon 0.02)\n",
            "576919:  313 games, mean reward 18.300, (epsilon 0.02)\n",
            "578857:  314 games, mean reward 18.290, (epsilon 0.02)\n",
            "580488:  315 games, mean reward 18.320, (epsilon 0.02)\n",
            "582853:  316 games, mean reward 18.300, (epsilon 0.02)\n",
            "584761:  317 games, mean reward 18.320, (epsilon 0.02)\n",
            "586696:  318 games, mean reward 18.340, (epsilon 0.02)\n",
            "588521:  319 games, mean reward 18.350, (epsilon 0.02)\n",
            "590997:  320 games, mean reward 18.260, (epsilon 0.02)\n",
            "593481:  321 games, mean reward 18.170, (epsilon 0.02)\n",
            "595113:  322 games, mean reward 18.180, (epsilon 0.02)\n",
            "596976:  323 games, mean reward 18.170, (epsilon 0.02)\n",
            "598773:  324 games, mean reward 18.220, (epsilon 0.02)\n",
            "600505:  325 games, mean reward 18.210, (epsilon 0.02)\n",
            "602321:  326 games, mean reward 18.290, (epsilon 0.02)\n",
            "604351:  327 games, mean reward 18.260, (epsilon 0.02)\n",
            "606096:  328 games, mean reward 18.270, (epsilon 0.02)\n",
            "607949:  329 games, mean reward 18.260, (epsilon 0.02)\n",
            "609894:  330 games, mean reward 18.240, (epsilon 0.02)\n",
            "611589:  331 games, mean reward 18.260, (epsilon 0.02)\n",
            "613274:  332 games, mean reward 18.260, (epsilon 0.02)\n",
            "615064:  333 games, mean reward 18.250, (epsilon 0.02)\n",
            "616825:  334 games, mean reward 18.250, (epsilon 0.02)\n",
            "618680:  335 games, mean reward 18.240, (epsilon 0.02)\n",
            "620921:  336 games, mean reward 18.180, (epsilon 0.02)\n",
            "623300:  337 games, mean reward 18.130, (epsilon 0.02)\n",
            "625030:  338 games, mean reward 18.110, (epsilon 0.02)\n",
            "626851:  339 games, mean reward 18.100, (epsilon 0.02)\n",
            "628604:  340 games, mean reward 18.140, (epsilon 0.02)\n",
            "630237:  341 games, mean reward 18.190, (epsilon 0.02)\n",
            "631901:  342 games, mean reward 18.190, (epsilon 0.02)\n",
            "633839:  343 games, mean reward 18.180, (epsilon 0.02)\n",
            "635474:  344 games, mean reward 18.200, (epsilon 0.02)\n",
            "637211:  345 games, mean reward 18.210, (epsilon 0.02)\n",
            "639291:  346 games, mean reward 18.170, (epsilon 0.02)\n",
            "641088:  347 games, mean reward 18.170, (epsilon 0.02)\n",
            "642857:  348 games, mean reward 18.170, (epsilon 0.02)\n",
            "644654:  349 games, mean reward 18.240, (epsilon 0.02)\n",
            "646501:  350 games, mean reward 18.270, (epsilon 0.02)\n",
            "648570:  351 games, mean reward 18.260, (epsilon 0.02)\n",
            "650550:  352 games, mean reward 18.240, (epsilon 0.02)\n",
            "652296:  353 games, mean reward 18.290, (epsilon 0.02)\n",
            "654135:  354 games, mean reward 18.290, (epsilon 0.02)\n",
            "655806:  355 games, mean reward 18.290, (epsilon 0.02)\n",
            "657441:  356 games, mean reward 18.300, (epsilon 0.02)\n",
            "659350:  357 games, mean reward 18.290, (epsilon 0.02)\n",
            "661347:  358 games, mean reward 18.300, (epsilon 0.02)\n",
            "663126:  359 games, mean reward 18.350, (epsilon 0.02)\n",
            "664758:  360 games, mean reward 18.430, (epsilon 0.02)\n",
            "666608:  361 games, mean reward 18.400, (epsilon 0.02)\n",
            "668273:  362 games, mean reward 18.400, (epsilon 0.02)\n",
            "669908:  363 games, mean reward 18.410, (epsilon 0.02)\n",
            "671698:  364 games, mean reward 18.420, (epsilon 0.02)\n",
            "673448:  365 games, mean reward 18.410, (epsilon 0.02)\n",
            "675471:  366 games, mean reward 18.420, (epsilon 0.02)\n",
            "677233:  367 games, mean reward 18.430, (epsilon 0.02)\n",
            "679025:  368 games, mean reward 18.440, (epsilon 0.02)\n",
            "680868:  369 games, mean reward 18.460, (epsilon 0.02)\n",
            "Best mean reward updated 18.460\n",
            "682863:  370 games, mean reward 18.420, (epsilon 0.02)\n",
            "684812:  371 games, mean reward 18.410, (epsilon 0.02)\n",
            "686447:  372 games, mean reward 18.450, (epsilon 0.02)\n",
            "688696:  373 games, mean reward 18.400, (epsilon 0.02)\n",
            "690612:  374 games, mean reward 18.370, (epsilon 0.02)\n",
            "692307:  375 games, mean reward 18.380, (epsilon 0.02)\n",
            "694037:  376 games, mean reward 18.380, (epsilon 0.02)\n",
            "695672:  377 games, mean reward 18.380, (epsilon 0.02)\n",
            "697306:  378 games, mean reward 18.430, (epsilon 0.02)\n",
            "699275:  379 games, mean reward 18.400, (epsilon 0.02)\n",
            "701100:  380 games, mean reward 18.440, (epsilon 0.02)\n",
            "702735:  381 games, mean reward 18.480, (epsilon 0.02)\n",
            "Best mean reward updated 18.480\n",
            "704753:  382 games, mean reward 18.420, (epsilon 0.02)\n",
            "706383:  383 games, mean reward 18.490, (epsilon 0.02)\n",
            "Best mean reward updated 18.490\n",
            "708347:  384 games, mean reward 18.470, (epsilon 0.02)\n",
            "710127:  385 games, mean reward 18.540, (epsilon 0.02)\n",
            "Best mean reward updated 18.540\n",
            "711921:  386 games, mean reward 18.550, (epsilon 0.02)\n",
            "Best mean reward updated 18.550\n",
            "714017:  387 games, mean reward 18.500, (epsilon 0.02)\n",
            "716113:  388 games, mean reward 18.440, (epsilon 0.02)\n",
            "717976:  389 games, mean reward 18.410, (epsilon 0.02)\n",
            "719813:  390 games, mean reward 18.450, (epsilon 0.02)\n",
            "721566:  391 games, mean reward 18.500, (epsilon 0.02)\n",
            "723522:  392 games, mean reward 18.520, (epsilon 0.02)\n",
            "725297:  393 games, mean reward 18.500, (epsilon 0.02)\n",
            "727922:  394 games, mean reward 18.360, (epsilon 0.02)\n",
            "729711:  395 games, mean reward 18.360, (epsilon 0.02)\n",
            "731347:  396 games, mean reward 18.370, (epsilon 0.02)\n",
            "733078:  397 games, mean reward 18.380, (epsilon 0.02)\n",
            "734857:  398 games, mean reward 18.410, (epsilon 0.02)\n",
            "736861:  399 games, mean reward 18.410, (epsilon 0.02)\n",
            "738858:  400 games, mean reward 18.390, (epsilon 0.02)\n",
            "740843:  401 games, mean reward 18.400, (epsilon 0.02)\n",
            "742479:  402 games, mean reward 18.420, (epsilon 0.02)\n",
            "744654:  403 games, mean reward 18.370, (epsilon 0.02)\n",
            "746500:  404 games, mean reward 18.360, (epsilon 0.02)\n",
            "748135:  405 games, mean reward 18.400, (epsilon 0.02)\n",
            "750017:  406 games, mean reward 18.360, (epsilon 0.02)\n",
            "751770:  407 games, mean reward 18.360, (epsilon 0.02)\n",
            "753587:  408 games, mean reward 18.410, (epsilon 0.02)\n",
            "755347:  409 games, mean reward 18.420, (epsilon 0.02)\n",
            "757046:  410 games, mean reward 18.430, (epsilon 0.02)\n",
            "758681:  411 games, mean reward 18.430, (epsilon 0.02)\n",
            "760774:  412 games, mean reward 18.500, (epsilon 0.02)\n",
            "762660:  413 games, mean reward 18.500, (epsilon 0.02)\n",
            "764354:  414 games, mean reward 18.530, (epsilon 0.02)\n",
            "765992:  415 games, mean reward 18.530, (epsilon 0.02)\n",
            "767628:  416 games, mean reward 18.600, (epsilon 0.02)\n",
            "Best mean reward updated 18.600\n",
            "769263:  417 games, mean reward 18.640, (epsilon 0.02)\n",
            "Best mean reward updated 18.640\n",
            "771576:  418 games, mean reward 18.600, (epsilon 0.02)\n",
            "773356:  419 games, mean reward 18.600, (epsilon 0.02)\n",
            "775008:  420 games, mean reward 18.720, (epsilon 0.02)\n",
            "Best mean reward updated 18.720\n",
            "776901:  421 games, mean reward 18.780, (epsilon 0.02)\n",
            "Best mean reward updated 18.780\n",
            "778686:  422 games, mean reward 18.770, (epsilon 0.02)\n",
            "780408:  423 games, mean reward 18.800, (epsilon 0.02)\n",
            "Best mean reward updated 18.800\n",
            "782483:  424 games, mean reward 18.780, (epsilon 0.02)\n",
            "784304:  425 games, mean reward 18.770, (epsilon 0.02)\n",
            "785939:  426 games, mean reward 18.770, (epsilon 0.02)\n",
            "787681:  427 games, mean reward 18.790, (epsilon 0.02)\n",
            "789499:  428 games, mean reward 18.780, (epsilon 0.02)\n",
            "791257:  429 games, mean reward 18.790, (epsilon 0.02)\n",
            "792892:  430 games, mean reward 18.830, (epsilon 0.02)\n",
            "Best mean reward updated 18.830\n",
            "794528:  431 games, mean reward 18.840, (epsilon 0.02)\n",
            "Best mean reward updated 18.840\n",
            "796161:  432 games, mean reward 18.850, (epsilon 0.02)\n",
            "Best mean reward updated 18.850\n",
            "798059:  433 games, mean reward 18.830, (epsilon 0.02)\n",
            "799860:  434 games, mean reward 18.820, (epsilon 0.02)\n",
            "801545:  435 games, mean reward 18.820, (epsilon 0.02)\n",
            "803333:  436 games, mean reward 18.870, (epsilon 0.02)\n",
            "Best mean reward updated 18.870\n",
            "805192:  437 games, mean reward 18.900, (epsilon 0.02)\n",
            "Best mean reward updated 18.900\n",
            "806875:  438 games, mean reward 18.910, (epsilon 0.02)\n",
            "Best mean reward updated 18.910\n",
            "808509:  439 games, mean reward 18.930, (epsilon 0.02)\n",
            "Best mean reward updated 18.930\n",
            "810291:  440 games, mean reward 18.930, (epsilon 0.02)\n",
            "811998:  441 games, mean reward 18.930, (epsilon 0.02)\n",
            "813703:  442 games, mean reward 18.920, (epsilon 0.02)\n",
            "815528:  443 games, mean reward 18.910, (epsilon 0.02)\n",
            "817735:  444 games, mean reward 18.800, (epsilon 0.02)\n",
            "819497:  445 games, mean reward 18.790, (epsilon 0.02)\n",
            "821486:  446 games, mean reward 18.800, (epsilon 0.02)\n",
            "823202:  447 games, mean reward 18.820, (epsilon 0.02)\n",
            "825374:  448 games, mean reward 18.760, (epsilon 0.02)\n",
            "827121:  449 games, mean reward 18.760, (epsilon 0.02)\n",
            "829261:  450 games, mean reward 18.700, (epsilon 0.02)\n",
            "831180:  451 games, mean reward 18.710, (epsilon 0.02)\n",
            "832813:  452 games, mean reward 18.740, (epsilon 0.02)\n",
            "834933:  453 games, mean reward 18.700, (epsilon 0.02)\n",
            "836568:  454 games, mean reward 18.710, (epsilon 0.02)\n",
            "838203:  455 games, mean reward 18.720, (epsilon 0.02)\n",
            "839947:  456 games, mean reward 18.710, (epsilon 0.02)\n",
            "841786:  457 games, mean reward 18.710, (epsilon 0.02)\n",
            "843569:  458 games, mean reward 18.730, (epsilon 0.02)\n",
            "845261:  459 games, mean reward 18.740, (epsilon 0.02)\n",
            "847044:  460 games, mean reward 18.730, (epsilon 0.02)\n",
            "848760:  461 games, mean reward 18.760, (epsilon 0.02)\n",
            "850492:  462 games, mean reward 18.760, (epsilon 0.02)\n",
            "852158:  463 games, mean reward 18.750, (epsilon 0.02)\n",
            "853943:  464 games, mean reward 18.730, (epsilon 0.02)\n",
            "856021:  465 games, mean reward 18.700, (epsilon 0.02)\n",
            "857844:  466 games, mean reward 18.710, (epsilon 0.02)\n",
            "859628:  467 games, mean reward 18.720, (epsilon 0.02)\n",
            "861573:  468 games, mean reward 18.710, (epsilon 0.02)\n",
            "863266:  469 games, mean reward 18.710, (epsilon 0.02)\n",
            "865275:  470 games, mean reward 18.720, (epsilon 0.02)\n",
            "867001:  471 games, mean reward 18.740, (epsilon 0.02)\n",
            "868724:  472 games, mean reward 18.730, (epsilon 0.02)\n",
            "870498:  473 games, mean reward 18.810, (epsilon 0.02)\n",
            "872314:  474 games, mean reward 18.810, (epsilon 0.02)\n",
            "874548:  475 games, mean reward 18.760, (epsilon 0.02)\n",
            "876565:  476 games, mean reward 18.740, (epsilon 0.02)\n",
            "878200:  477 games, mean reward 18.740, (epsilon 0.02)\n",
            "879833:  478 games, mean reward 18.740, (epsilon 0.02)\n",
            "882128:  479 games, mean reward 18.710, (epsilon 0.02)\n",
            "883952:  480 games, mean reward 18.710, (epsilon 0.02)\n",
            "885638:  481 games, mean reward 18.700, (epsilon 0.02)\n",
            "887499:  482 games, mean reward 18.740, (epsilon 0.02)\n",
            "889134:  483 games, mean reward 18.740, (epsilon 0.02)\n",
            "891160:  484 games, mean reward 18.740, (epsilon 0.02)\n",
            "893012:  485 games, mean reward 18.760, (epsilon 0.02)\n",
            "894830:  486 games, mean reward 18.770, (epsilon 0.02)\n",
            "896821:  487 games, mean reward 18.780, (epsilon 0.02)\n",
            "898563:  488 games, mean reward 18.830, (epsilon 0.02)\n",
            "900326:  489 games, mean reward 18.870, (epsilon 0.02)\n",
            "902048:  490 games, mean reward 18.880, (epsilon 0.02)\n",
            "903684:  491 games, mean reward 18.880, (epsilon 0.02)\n",
            "905395:  492 games, mean reward 18.920, (epsilon 0.02)\n",
            "907030:  493 games, mean reward 18.940, (epsilon 0.02)\n",
            "Best mean reward updated 18.940\n",
            "908662:  494 games, mean reward 19.090, (epsilon 0.02)\n",
            "Best mean reward updated 19.090\n",
            "Solved in 908662 frames!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import collections\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "FPS = 25"
      ],
      "metadata": {
        "id": "2lgz522sZ8bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]\n",
        "\n",
        "import pyvirtualdisplay"
      ],
      "metadata": {
        "id": "XxdxBW0XbB2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install moviepy"
      ],
      "metadata": {
        "id": "xWjhicSZbFkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(DEFAULT_ENV_NAME, render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(env, 'video', episode_trigger = lambda episode_id: True)"
      ],
      "metadata": {
        "id": "pO2w-uNabHxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import collections\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "FPS = 25\n",
        "\n",
        "model='PongNoFrameskip-v4-best.dat'\n",
        "record_folder=\"video\"\n",
        "visualize=True\n",
        "\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "if record_folder:\n",
        "        env = gym.wrappers.RecordVideo(env, record_folder,  episode_trigger = lambda episode_id: True)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        if visualize:\n",
        "            env.render()\n",
        "        state_v = torch.tensor(np.array([state], copy=False))\n",
        "        q_vals = net(state_v).data.numpy()[0]\n",
        "        action = np.argmax(q_vals)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "        if visualize:\n",
        "            delta = 1/FPS - (time.time() - start_ts)\n",
        "            if delta > 0:\n",
        "                time.sleep(delta)\n",
        "print(\"Total reward: %.2f\" % total_reward)"
      ],
      "metadata": {
        "id": "1bey-MT1bKBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}