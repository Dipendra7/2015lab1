{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfRhAgiE5HVXthG3Kz3F/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipendra7/2015lab1/blob/master/Pong_NN_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycpQ1no-k0i4",
        "outputId": "feb93de6-9b7e-4614-d34c-d8c78551d88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Requirement already satisfied: keras==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.11.0\n",
        "!pip install keras==2.11.0\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gym[atari, accept-rom-license]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAUyZ63Bk_NS",
        "outputId": "1070384f-3a31-404d-930c-170f031e1585"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (6.4.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (4.66.5)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2024.7.4)\n",
            "Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=ad1a416f9b7faadb9d7b0885c40a5fff67231c32f2a5688fcdb852511faa0c16\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def downsample(image):\n",
        "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
        "    return image[::2, ::2, :]\n",
        "\n",
        "def remove_color(image):\n",
        "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
        "    return image[:, :, 0]\n",
        "\n",
        "def remove_background(image):\n",
        "    image[image == 144] = 0\n",
        "    image[image == 109] = 0\n",
        "    return image"
      ],
      "metadata": {
        "id": "c6nTSYG0lfhc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
        "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
        "    processed_observation = input_observation[35:195] # crop\n",
        "    processed_observation = downsample(processed_observation)\n",
        "    processed_observation = remove_color(processed_observation)\n",
        "    processed_observation = remove_background(processed_observation)\n",
        "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
        "    processed_observation = processed_observation.astype(float).ravel()\n",
        "\n",
        "    # subtract the previous frame from the current one so we are only processing on changes in the game\n",
        "    if prev_processed_observation is not None:\n",
        "        input_observation = processed_observation - prev_processed_observation\n",
        "    else:\n",
        "        input_observation = np.zeros(input_dimensions)\n",
        "    # store the previous frame so we can subtract from it next time\n",
        "    prev_processed_observations = processed_observation\n",
        "    return input_observation, prev_processed_observations\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def relu(vector):\n",
        "    vector[vector < 0] = 0\n",
        "    return vector\n",
        "\n",
        "def apply_neural_nets(observation_matrix, weights):\n",
        "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
        "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
        "    hidden_layer_values = relu(hidden_layer_values)\n",
        "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
        "    output_layer_values = sigmoid(output_layer_values)\n",
        "    return hidden_layer_values, output_layer_values\n",
        "\n",
        "def choose_action(probability):\n",
        "    random_value = np.random.uniform()\n",
        "    if random_value < probability:\n",
        "        # signifies up in openai gym\n",
        "        return 2\n",
        "    else:\n",
        "         # signifies down in openai gym\n",
        "        return 3"
      ],
      "metadata": {
        "id": "YU2o_Npgli8k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
        "    \"\"\" See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
        "    delta_L = gradient_log_p\n",
        "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
        "    delta_l2 = np.outer(delta_L, weights['2'])\n",
        "    delta_l2 = relu(delta_l2)\n",
        "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
        "    return {\n",
        "        '1': dC_dw1,\n",
        "        '2': dC_dw2\n",
        "    }\n",
        "\n",
        "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
        "    \"\"\" See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
        "    epsilon = 1e-5\n",
        "    for layer_name in weights.keys():\n",
        "        g = g_dict[layer_name]\n",
        "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
        "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
        "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
        "\n",
        "def discount_rewards(rewards, gamma):\n",
        "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took a step ago.\n",
        "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, rewards.size)):\n",
        "        if rewards[t] != 0:\n",
        "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "        running_add = running_add * gamma + rewards[t]\n",
        "        discounted_rewards[t] = running_add\n",
        "    return discounted_rewards\n",
        "\n",
        "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
        "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
        "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
        "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
        "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
        "    return gradient_log_p * discounted_episode_rewards\n"
      ],
      "metadata": {
        "id": "ehXdJ8TBllzt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make(\"Pong-v0\", render_mode='rgb_array')\n",
        "    observation = env.reset() # This gets us the image\n",
        "    #observation = env.reset() # This gets us the image\n",
        "\n",
        "    # hyperparameters\n",
        "    episode_number = 0\n",
        "    batch_size = 10\n",
        "    gamma = 0.99 # discount factor for reward\n",
        "    decay_rate = 0.99\n",
        "    num_hidden_layer_neurons = 200\n",
        "    input_dimensions = 80 * 80\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    episode_number = 0\n",
        "    reward_sum = 0\n",
        "    running_reward = None\n",
        "    prev_processed_observations = None\n",
        "\n",
        "    weights = {\n",
        "        '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
        "        '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
        "    }\n",
        "\n",
        "    # To be used with rmsprop algorithm (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
        "    expectation_g_squared = {}\n",
        "    g_dict = {}\n",
        "    for layer_name in weights.keys():\n",
        "        expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
        "        g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
        "\n",
        "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
        "\n",
        "\n",
        "    while True:\n",
        "        env.render(mode='rgb_array')\n",
        "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
        "        hidden_layer_values, up_probability = apply_neural_nets(processed_observations, weights)\n",
        "\n",
        "        episode_observations.append(processed_observations)\n",
        "        episode_hidden_layer_values.append(hidden_layer_values)\n",
        "\n",
        "        action = choose_action(up_probability)\n",
        "\n",
        "        # carry out the chosen action\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        reward_sum += reward\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
        "        fake_label = 1 if action == 2 else 0\n",
        "        loss_function_gradient = fake_label - up_probability\n",
        "        episode_gradient_log_ps.append(loss_function_gradient)\n",
        "\n",
        "\n",
        "        if done: # an episode finished\n",
        "            episode_number += 1\n",
        "\n",
        "            # Combine the following values for the episode\n",
        "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
        "            episode_observations = np.vstack(episode_observations)\n",
        "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
        "            episode_rewards = np.vstack(episode_rewards)\n",
        "\n",
        "            # Tweak the gradient of the log_ps based on the discounted rewards\n",
        "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
        "\n",
        "            gradient = compute_gradient(\n",
        "              episode_gradient_log_ps_discounted,\n",
        "              episode_hidden_layer_values,\n",
        "              episode_observations,\n",
        "              weights\n",
        "            )\n",
        "\n",
        "            # Sum the gradient for use when we hit the batch size\n",
        "            for layer_name in gradient:\n",
        "                g_dict[layer_name] += gradient[layer_name]\n",
        "\n",
        "            if episode_number % batch_size == 0:\n",
        "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
        "\n",
        "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
        "            observation = env.reset() # reset env\n",
        "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "            print ('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
        "            reward_sum = 0\n",
        "            prev_processed_observations = None\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc7rgZyolol8",
        "outputId": "50c76bf3-8dc0-4895-861f-caec9a79f5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.980397\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.960593\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.960987\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.951377\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.941863\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.932445\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.933120\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.933789\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.924451\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.925207\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.925955\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.916695\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.907528\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.878453\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.879668\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.860872\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.862263\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.863640\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.855004\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.846454\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.837989\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.819609\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.821413\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.823199\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.824967\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.816718\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.818550\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.820365\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.822161\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.823940\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.825700\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.797443\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.799469\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.791474\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.793559\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.775624\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.767868\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.770189\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.762487\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.744862\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.747414\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.749939\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.732440\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.735116\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.737764\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.730387\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.723083\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.715852\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.718694\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.721507\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.704292\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.687249\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.690376\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.683472\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.656638\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.660071\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.663471\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.656836\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.660268\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.663665\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.667028\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.650358\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.633854\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.637516\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.631141\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.634829\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.638481\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.632096\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.635775\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.619417\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.613223\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.617091\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.620920\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.624711\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.628464\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.632179\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.635857\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.629499\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.633204\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.636872\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.640503\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.634098\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.637757\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.641379\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.634966\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.638616\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.642230\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.635808\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.639449\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.643055\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.646624\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.650158\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.643657\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.637220\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.640848\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.644439\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.647995\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.651515\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.645000\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.648550\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.622064\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.625844\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.629585\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.623289\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.627057\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.630786\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.634478\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.628133\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.631852\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.615533\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.619378\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.623184\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.616953\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.600783\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.584775\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.578927\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.583138\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.587307\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.591434\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.595519\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.589564\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.593669\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.597732\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.591755\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.595837\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.599879\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.603880\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.607841\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.611763\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.605645\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.599589\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.583593\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.587757\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.581879\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.576060\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.570300\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.574597\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.578851\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.563062\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.567432\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.571757\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.576040\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.570279\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.574577\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.578831\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.583043\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.577212\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.581440\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.555626\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.540069\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.534669\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.539322\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.543929\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.548489\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.553005\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.547474\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.542000\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.546580\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.541114\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.535703\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.540346\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.524942\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.519693\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.524496\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.519251\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.494058\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.489118\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.494227\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.499284\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.494292\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.479349\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.464555\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.469910\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.475211\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.470458\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.475754\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.480996\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.486186\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.491325\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.496411\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.481447\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.486633\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.481766\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.476949\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.482179\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.487357\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.492484\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.497559\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.502583\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507558\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.502482\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507457\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.492383\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.497459\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.502484\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507459\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.502385\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.497361\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.492387\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.497463\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.502489\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507464\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.502389\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507365\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.512292\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.517169\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.511997\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.516877\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.521708\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.526491\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.521226\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.506014\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.500954\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.505944\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.510885\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.505776\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.510718\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.505611\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.510555\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.505450\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.510395\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.505291\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.500238\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.505236\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.490183\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.485282\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.480429\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.485624\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.490768\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.485861\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.481002\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.476192\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.471430\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.476716\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.481949\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.487129\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.492258\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.487335\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.492462\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.467537\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.462862\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.468233\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.473551\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.468815\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.454127\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.459586\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.464990\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.460340\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.465737\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.461079\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.466469\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.461804\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.467186\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.472514\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.467789\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.463111\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.458480\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.463895\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.469256\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.474564\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.479818\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.475020\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.480270\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.485467\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.490612\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.475706\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.480949\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.476140\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.481378\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.466564\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.471899\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.477180\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.482408\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.487584\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.492708\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.497781\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.502803\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.497775\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.502797\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.507769\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.512692\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.507565\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.512489\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.517364\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.522191\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.516969\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.511799\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.496681\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.471714\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.456997\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.462427\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.467803\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.463125\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.458494\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.463909\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.469270\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.474577\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.479831\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.485033\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.490182\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.495281\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.470328\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.475625\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.480868\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.486060\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.481199\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.476387\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.481623\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.486807\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.461939\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.467319\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.472646\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.477920\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.483141\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.478309\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.483526\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.488691\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.483804\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.488966\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.494076\n",
            "resetting env. episode reward total was -18.000000. running mean: -20.469135\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.464444\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.469800\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.475102\n",
            "resetting env. episode reward total was -20.000000. running mean: -20.470351\n",
            "resetting env. episode reward total was -19.000000. running mean: -20.455647\n",
            "resetting env. episode reward total was -21.000000. running mean: -20.461091\n"
          ]
        }
      ]
    }
  ]
}