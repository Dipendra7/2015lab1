{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipendra7/2015lab1/blob/master/Copy_Pong_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DGro4Gt9zvo",
        "outputId": "5eca5483-0d76-4a53-aa49-ea4fbcc43326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 14 20:01:28 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0              25W /  70W |    313MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (6.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "test_env = gym.make(DEFAULT_ENV_NAME)\n",
        "print(test_env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsqdUcM1_iRC",
        "outputId": "afef6b9a-d76a-4557-ffcf-ad7404b8994b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_env.unwrapped.get_action_meanings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie_3HVpg_o08",
        "outputId": "e6611bab-0206-42b5-cded-fa28ec4e51c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgUQYNV_qoK",
        "outputId": "b2f07d61-e199-4820-842b-078b6a3d67d7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ezGUJ_P_snz",
        "outputId": "f87a393a-fe0d-4743-8074-db2dbfc8545e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 14 20:01:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0              25W /  70W |    313MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VGyUSX3B_vL2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1],\n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "O8OUg1X9_xOQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn        # Pytorch neural network package\n",
        "import torch.optim as optim  # Pytorch optimization package\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "kV40hCBn_zvK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "wRYdCmum_1nQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = make_env(DEFAULT_ENV_NAME)\n",
        "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
        "print(test_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upZJ0LAN_4Zn",
        "outputId": "43710e3f-a51f-42fb-c972-5b0a4f344eed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SNN Implementation"
      ],
      "metadata": {
        "id": "kvUMLaf1VT2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfXZmkhZtO1D",
        "outputId": "f4dc91fb-bd89-4a10-ec91-56daafd4e65f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.4.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Requirement already satisfied: nir in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0.4)\n",
            "Requirement already satisfied: nirtorch in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CFaxUU-ItMfq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 1"
      ],
      "metadata": {
        "id": "hPxyJSEmVhNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspired From\n",
        "\n",
        "#https://github.com/sofi12321/SNN_image_classification/blob/main/SNN_image_classification.ipynb\n",
        "\n",
        "\n",
        "class SNN_Arch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs,\n",
        "                 num_hidden=1024, num_steps=25,\n",
        "                 beta=0.95):\n",
        "        \"\"\"\n",
        "        SNN with one hidden layer\n",
        "\n",
        "        :param num_inputs: number of input units\n",
        "        :param num_outputs: number of output units\n",
        "        :param num_hidden: number of hidden units\n",
        "        :param num_steps: number of time steps\n",
        "        :param beta: beta coefficient value for Leaky model\n",
        "        \"\"\"\n",
        "        super(SNN_Arch, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        # Initialize number of time steps\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward step\n",
        "\n",
        "        :param x: input values\n",
        "        :return: output spikes and membrane potential\n",
        "        \"\"\"\n",
        "        # Initialize hidden states at t=0\n",
        "        potential1 = self.lif1.init_leaky()\n",
        "        potential2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the output values\n",
        "        output_spike = []\n",
        "        output_potential = []\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            # For each time step run through the SNN\n",
        "            current1 = self.linear1(x)\n",
        "            spike1, potential1 = self.lif1(current1, potential1)\n",
        "            current2 = self.linear2(spike1)\n",
        "            spike2, potential2 = self.lif2(current2, potential2)\n",
        "\n",
        "            # Record outputs\n",
        "            output_spike.append(spike2)\n",
        "            output_potential.append(potential2)\n",
        "        return torch.stack(output_spike, dim=0), torch.stack(output_potential, dim=0)\n",
        "\n",
        "\n",
        "# SNN\n",
        "class SNN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions,\n",
        "                 num_hidden=1024, num_steps=25, beta=0.95):\n",
        "        \"\"\"\n",
        "        DQN architecture using Spiking Neural Network (SNN)\n",
        "        :param input_shape: shape of the input\n",
        "        :param n_actions: number of output actions\n",
        "        \"\"\"\n",
        "        super(SNN, self).__init__()\n",
        "\n",
        "        # Flatten input from the Conv layers into a 1D input vector for SNN\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # SNN replacing the fully connected layers in DQN\n",
        "        self.snn = SNN_Arch(conv_out_size, n_actions, num_hidden, num_steps, beta)\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        # Function to compute the output size of the conv layers\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the conv layers\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "\n",
        "        # Pass through the SNN\n",
        "        spike_out, potential_out = self.snn(conv_out)\n",
        "\n",
        "        # Return the final membrane potential (last timestep) as output\n",
        "        return potential_out[-1]\n"
      ],
      "metadata": {
        "id": "j27ElO0wuYP0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "test_env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "# Define the input and output shapes\n",
        "input_shape = test_env.observation_space.shape  # Same input shape as the DQN\n",
        "n_actions = test_env.action_space.n             # Number of possible actions\n",
        "\n",
        "# Instantiate the SNN model (you can adjust num_hidden, num_steps, and beta as needed)\n",
        "test_net = SNN(input_shape, n_actions, num_hidden=128, num_steps=10, beta=0.9).to(device)\n",
        "\n",
        "# Print the network architecture\n",
        "print(test_net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj-X2qM-uziC",
        "outputId": "eff7812f-4ab6-4a05-d5e3-f22a4fb817a5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (snn): SNN_Arch(\n",
            "    (linear1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "    (lif1): Leaky()\n",
            "    (linear2): Linear(in_features=128, out_features=6, bias=True)\n",
            "    (lif2): Leaky()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "48l3bY8SAQy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb5a0f8-38e7-40d9-c75e-fae97d2bd760"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "MEAN_REWARD_BOUND = 19.0\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 32\n",
        "replay_size = 10000\n",
        "learning_rate = 1e-4\n",
        "sync_target_frames = 1000\n",
        "replay_start_size = 10000\n",
        "\n",
        "eps_start=1.0\n",
        "eps_decay=.999985\n",
        "eps_min=0.02"
      ],
      "metadata": {
        "id": "BIcESgTJATVr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "metadata": {
        "id": "wxiRAdYLAVx9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "KIwwK64qAYHU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cItdcabnAa_d",
        "outputId": "a268c1c9-3054-4f92-f601-29fe315c9e27"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>Training starts at  2024-09-14 20:01:42.874119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "# net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "\n",
        "net = SNN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "target_net = SNN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "\n",
        "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
        "\n",
        "buffer = ExperienceReplay(replay_size)\n",
        "agent = Agent(env, buffer)\n",
        "\n",
        "epsilon = eps_start\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "\n",
        "best_mean_reward = None\n",
        "\n",
        "while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon*eps_decay, eps_min)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
        "\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        states_v = torch.tensor(states).to(device)\n",
        "        next_states_v = torch.tensor(next_states).to(device)\n",
        "        actions_v = torch.tensor(actions).to(device)\n",
        "        rewards_v = torch.tensor(rewards).to(device)\n",
        "        #done_mask = torch.ByteTensor(dones).to(device)\n",
        "        done_mask = torch.BoolTensor(dones).to(device) # Change to BoolTensor instead of ByteTensor\n",
        "\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        next_state_values[done_mask] = 0.0\n",
        "\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % sync_target_frames == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZfaVLJrAc9v",
        "outputId": "967a45eb-6dbf-48c1-f21a-b4518684d110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>Training starts at  2024-09-14 20:01:42.895443\n",
            "924:  1 games, mean reward -20.000, (epsilon 0.99)\n",
            "Best mean reward updated -20.000\n",
            "1875:  2 games, mean reward -20.000, (epsilon 0.97)\n",
            "2896:  3 games, mean reward -20.000, (epsilon 0.96)\n",
            "3705:  4 games, mean reward -20.250, (epsilon 0.95)\n",
            "4616:  5 games, mean reward -20.400, (epsilon 0.93)\n",
            "5499:  6 games, mean reward -20.500, (epsilon 0.92)\n",
            "6322:  7 games, mean reward -20.571, (epsilon 0.91)\n",
            "7113:  8 games, mean reward -20.625, (epsilon 0.90)\n",
            "8056:  9 games, mean reward -20.667, (epsilon 0.89)\n",
            "8819:  10 games, mean reward -20.700, (epsilon 0.88)\n",
            "9610:  11 games, mean reward -20.727, (epsilon 0.87)\n",
            "10373:  12 games, mean reward -20.750, (epsilon 0.86)\n",
            "11258:  13 games, mean reward -20.769, (epsilon 0.84)\n",
            "12049:  14 games, mean reward -20.786, (epsilon 0.83)\n",
            "12951:  15 games, mean reward -20.800, (epsilon 0.82)\n",
            "14115:  16 games, mean reward -20.688, (epsilon 0.81)\n",
            "15125:  17 games, mean reward -20.647, (epsilon 0.80)\n",
            "15888:  18 games, mean reward -20.667, (epsilon 0.79)\n",
            "16729:  19 games, mean reward -20.684, (epsilon 0.78)\n",
            "17887:  20 games, mean reward -20.600, (epsilon 0.76)\n",
            "19016:  21 games, mean reward -20.524, (epsilon 0.75)\n",
            "19844:  22 games, mean reward -20.545, (epsilon 0.74)\n",
            "20696:  23 games, mean reward -20.565, (epsilon 0.73)\n",
            "21702:  24 games, mean reward -20.583, (epsilon 0.72)\n",
            "22465:  25 games, mean reward -20.600, (epsilon 0.71)\n",
            "23348:  26 games, mean reward -20.615, (epsilon 0.70)\n",
            "24405:  27 games, mean reward -20.593, (epsilon 0.69)\n",
            "25319:  28 games, mean reward -20.607, (epsilon 0.68)\n",
            "26399:  29 games, mean reward -20.552, (epsilon 0.67)\n",
            "27373:  30 games, mean reward -20.533, (epsilon 0.66)\n",
            "28154:  31 games, mean reward -20.548, (epsilon 0.66)\n",
            "29201:  32 games, mean reward -20.562, (epsilon 0.65)\n",
            "30173:  33 games, mean reward -20.515, (epsilon 0.64)\n",
            "31178:  34 games, mean reward -20.500, (epsilon 0.63)\n",
            "32033:  35 games, mean reward -20.514, (epsilon 0.62)\n",
            "33247:  36 games, mean reward -20.500, (epsilon 0.61)\n",
            "34315:  37 games, mean reward -20.459, (epsilon 0.60)\n",
            "35240:  38 games, mean reward -20.474, (epsilon 0.59)\n",
            "36197:  39 games, mean reward -20.462, (epsilon 0.58)\n",
            "37435:  40 games, mean reward -20.450, (epsilon 0.57)\n",
            "38510:  41 games, mean reward -20.415, (epsilon 0.56)\n",
            "39721:  42 games, mean reward -20.357, (epsilon 0.55)\n",
            "40883:  43 games, mean reward -20.302, (epsilon 0.54)\n",
            "42206:  44 games, mean reward -20.273, (epsilon 0.53)\n",
            "43059:  45 games, mean reward -20.289, (epsilon 0.52)\n",
            "43878:  46 games, mean reward -20.304, (epsilon 0.52)\n",
            "44904:  47 games, mean reward -20.277, (epsilon 0.51)\n",
            "46007:  48 games, mean reward -20.250, (epsilon 0.50)\n",
            "46997:  49 games, mean reward -20.245, (epsilon 0.49)\n",
            "48011:  50 games, mean reward -20.240, (epsilon 0.49)\n",
            "48923:  51 games, mean reward -20.255, (epsilon 0.48)\n",
            "49823:  52 games, mean reward -20.250, (epsilon 0.47)\n",
            "50815:  53 games, mean reward -20.264, (epsilon 0.47)\n",
            "51727:  54 games, mean reward -20.278, (epsilon 0.46)\n",
            "52807:  55 games, mean reward -20.255, (epsilon 0.45)\n",
            "53694:  56 games, mean reward -20.268, (epsilon 0.45)\n",
            "54618:  57 games, mean reward -20.263, (epsilon 0.44)\n",
            "55701:  58 games, mean reward -20.259, (epsilon 0.43)\n",
            "56948:  59 games, mean reward -20.220, (epsilon 0.43)\n",
            "57788:  60 games, mean reward -20.217, (epsilon 0.42)\n",
            "59003:  61 games, mean reward -20.197, (epsilon 0.41)\n",
            "60140:  62 games, mean reward -20.194, (epsilon 0.41)\n",
            "61098:  63 games, mean reward -20.190, (epsilon 0.40)\n",
            "62018:  64 games, mean reward -20.188, (epsilon 0.39)\n",
            "62897:  65 games, mean reward -20.200, (epsilon 0.39)\n",
            "63863:  66 games, mean reward -20.197, (epsilon 0.38)\n",
            "64654:  67 games, mean reward -20.209, (epsilon 0.38)\n",
            "65496:  68 games, mean reward -20.221, (epsilon 0.37)\n",
            "66629:  69 games, mean reward -20.203, (epsilon 0.37)\n",
            "67721:  70 games, mean reward -20.214, (epsilon 0.36)\n",
            "68606:  71 games, mean reward -20.225, (epsilon 0.36)\n",
            "69939:  72 games, mean reward -20.194, (epsilon 0.35)\n",
            "71264:  73 games, mean reward -20.164, (epsilon 0.34)\n",
            "72240:  74 games, mean reward -20.162, (epsilon 0.34)\n",
            "73291:  75 games, mean reward -20.160, (epsilon 0.33)\n",
            "74138:  76 games, mean reward -20.171, (epsilon 0.33)\n",
            "75049:  77 games, mean reward -20.182, (epsilon 0.32)\n",
            "75859:  78 games, mean reward -20.192, (epsilon 0.32)\n",
            "76815:  79 games, mean reward -20.190, (epsilon 0.32)\n",
            "77709:  80 games, mean reward -20.188, (epsilon 0.31)\n",
            "79191:  81 games, mean reward -20.160, (epsilon 0.30)\n",
            "80260:  82 games, mean reward -20.159, (epsilon 0.30)\n",
            "81360:  83 games, mean reward -20.133, (epsilon 0.30)\n",
            "82196:  84 games, mean reward -20.131, (epsilon 0.29)\n",
            "83153:  85 games, mean reward -20.129, (epsilon 0.29)\n",
            "84362:  86 games, mean reward -20.105, (epsilon 0.28)\n",
            "85247:  87 games, mean reward -20.115, (epsilon 0.28)\n",
            "86314:  88 games, mean reward -20.114, (epsilon 0.27)\n",
            "87333:  89 games, mean reward -20.101, (epsilon 0.27)\n",
            "88218:  90 games, mean reward -20.111, (epsilon 0.27)\n",
            "89216:  91 games, mean reward -20.110, (epsilon 0.26)\n",
            "90320:  92 games, mean reward -20.098, (epsilon 0.26)\n",
            "91185:  93 games, mean reward -20.108, (epsilon 0.25)\n",
            "92228:  94 games, mean reward -20.096, (epsilon 0.25)\n",
            "93227:  95 games, mean reward -20.105, (epsilon 0.25)\n",
            "94218:  96 games, mean reward -20.104, (epsilon 0.24)\n",
            "95069:  97 games, mean reward -20.113, (epsilon 0.24)\n",
            "96023:  98 games, mean reward -20.112, (epsilon 0.24)\n",
            "97157:  99 games, mean reward -20.101, (epsilon 0.23)\n",
            "98185:  100 games, mean reward -20.100, (epsilon 0.23)\n",
            "99285:  101 games, mean reward -20.100, (epsilon 0.23)\n",
            "100300:  102 games, mean reward -20.100, (epsilon 0.22)\n",
            "101215:  103 games, mean reward -20.100, (epsilon 0.22)\n",
            "102338:  104 games, mean reward -20.080, (epsilon 0.22)\n",
            "103455:  105 games, mean reward -20.060, (epsilon 0.21)\n",
            "104573:  106 games, mean reward -20.050, (epsilon 0.21)\n",
            "105891:  107 games, mean reward -20.020, (epsilon 0.20)\n",
            "107027:  108 games, mean reward -20.000, (epsilon 0.20)\n",
            "108087:  109 games, mean reward -19.990, (epsilon 0.20)\n",
            "Best mean reward updated -19.990\n",
            "109050:  110 games, mean reward -19.990, (epsilon 0.19)\n",
            "110209:  111 games, mean reward -19.970, (epsilon 0.19)\n",
            "Best mean reward updated -19.970\n",
            "111249:  112 games, mean reward -19.970, (epsilon 0.19)\n",
            "112379:  113 games, mean reward -19.970, (epsilon 0.19)\n",
            "113533:  114 games, mean reward -19.950, (epsilon 0.18)\n",
            "Best mean reward updated -19.950\n",
            "114459:  115 games, mean reward -19.940, (epsilon 0.18)\n",
            "Best mean reward updated -19.940\n",
            "115414:  116 games, mean reward -19.960, (epsilon 0.18)\n",
            "116577:  117 games, mean reward -19.960, (epsilon 0.17)\n",
            "117865:  118 games, mean reward -19.930, (epsilon 0.17)\n",
            "Best mean reward updated -19.930\n",
            "118810:  119 games, mean reward -19.930, (epsilon 0.17)\n",
            "119909:  120 games, mean reward -19.920, (epsilon 0.17)\n",
            "Best mean reward updated -19.920\n",
            "121012:  121 games, mean reward -19.940, (epsilon 0.16)\n",
            "122343:  122 games, mean reward -19.910, (epsilon 0.16)\n",
            "Best mean reward updated -19.910\n",
            "123468:  123 games, mean reward -19.900, (epsilon 0.16)\n",
            "Best mean reward updated -19.900\n",
            "124670:  124 games, mean reward -19.900, (epsilon 0.15)\n",
            "126083:  125 games, mean reward -19.890, (epsilon 0.15)\n",
            "Best mean reward updated -19.890\n",
            "127740:  126 games, mean reward -19.830, (epsilon 0.15)\n",
            "Best mean reward updated -19.830\n",
            "129104:  127 games, mean reward -19.830, (epsilon 0.14)\n",
            "130297:  128 games, mean reward -19.810, (epsilon 0.14)\n",
            "Best mean reward updated -19.810\n",
            "131572:  129 games, mean reward -19.800, (epsilon 0.14)\n",
            "Best mean reward updated -19.800\n",
            "133135:  130 games, mean reward -19.760, (epsilon 0.14)\n",
            "Best mean reward updated -19.760\n",
            "134529:  131 games, mean reward -19.760, (epsilon 0.13)\n",
            "136157:  132 games, mean reward -19.740, (epsilon 0.13)\n",
            "Best mean reward updated -19.740\n",
            "137748:  133 games, mean reward -19.740, (epsilon 0.13)\n",
            "139007:  134 games, mean reward -19.720, (epsilon 0.12)\n",
            "Best mean reward updated -19.720\n",
            "140333:  135 games, mean reward -19.700, (epsilon 0.12)\n",
            "Best mean reward updated -19.700\n",
            "141882:  136 games, mean reward -19.670, (epsilon 0.12)\n",
            "Best mean reward updated -19.670\n",
            "143211:  137 games, mean reward -19.680, (epsilon 0.12)\n",
            "144353:  138 games, mean reward -19.650, (epsilon 0.11)\n",
            "Best mean reward updated -19.650\n",
            "145710:  139 games, mean reward -19.640, (epsilon 0.11)\n",
            "Best mean reward updated -19.640\n",
            "147124:  140 games, mean reward -19.590, (epsilon 0.11)\n",
            "Best mean reward updated -19.590\n",
            "148299:  141 games, mean reward -19.600, (epsilon 0.11)\n",
            "149388:  142 games, mean reward -19.620, (epsilon 0.11)\n",
            "150556:  143 games, mean reward -19.640, (epsilon 0.10)\n",
            "151947:  144 games, mean reward -19.630, (epsilon 0.10)\n",
            "152963:  145 games, mean reward -19.630, (epsilon 0.10)\n",
            "154330:  146 games, mean reward -19.620, (epsilon 0.10)\n",
            "155445:  147 games, mean reward -19.640, (epsilon 0.10)\n",
            "156659:  148 games, mean reward -19.630, (epsilon 0.10)\n",
            "158036:  149 games, mean reward -19.610, (epsilon 0.09)\n",
            "159594:  150 games, mean reward -19.580, (epsilon 0.09)\n",
            "Best mean reward updated -19.580\n",
            "161023:  151 games, mean reward -19.560, (epsilon 0.09)\n",
            "Best mean reward updated -19.560\n",
            "162325:  152 games, mean reward -19.550, (epsilon 0.09)\n",
            "Best mean reward updated -19.550\n",
            "163698:  153 games, mean reward -19.530, (epsilon 0.09)\n",
            "Best mean reward updated -19.530\n",
            "164961:  154 games, mean reward -19.520, (epsilon 0.08)\n",
            "Best mean reward updated -19.520\n",
            "166126:  155 games, mean reward -19.540, (epsilon 0.08)\n",
            "167417:  156 games, mean reward -19.510, (epsilon 0.08)\n",
            "Best mean reward updated -19.510\n",
            "168845:  157 games, mean reward -19.500, (epsilon 0.08)\n",
            "Best mean reward updated -19.500\n",
            "170451:  158 games, mean reward -19.480, (epsilon 0.08)\n",
            "Best mean reward updated -19.480\n",
            "172042:  159 games, mean reward -19.480, (epsilon 0.08)\n",
            "173768:  160 games, mean reward -19.460, (epsilon 0.07)\n",
            "Best mean reward updated -19.460\n",
            "175277:  161 games, mean reward -19.460, (epsilon 0.07)\n",
            "176814:  162 games, mean reward -19.450, (epsilon 0.07)\n",
            "Best mean reward updated -19.450\n",
            "178311:  163 games, mean reward -19.410, (epsilon 0.07)\n",
            "Best mean reward updated -19.410\n",
            "179623:  164 games, mean reward -19.400, (epsilon 0.07)\n",
            "Best mean reward updated -19.400\n",
            "181280:  165 games, mean reward -19.360, (epsilon 0.07)\n",
            "Best mean reward updated -19.360\n",
            "182812:  166 games, mean reward -19.350, (epsilon 0.06)\n",
            "Best mean reward updated -19.350\n",
            "184185:  167 games, mean reward -19.310, (epsilon 0.06)\n",
            "Best mean reward updated -19.310\n",
            "185768:  168 games, mean reward -19.280, (epsilon 0.06)\n",
            "Best mean reward updated -19.280\n",
            "187057:  169 games, mean reward -19.280, (epsilon 0.06)\n",
            "188468:  170 games, mean reward -19.250, (epsilon 0.06)\n",
            "Best mean reward updated -19.250\n",
            "190014:  171 games, mean reward -19.190, (epsilon 0.06)\n",
            "Best mean reward updated -19.190\n",
            "191524:  172 games, mean reward -19.170, (epsilon 0.06)\n",
            "Best mean reward updated -19.170\n",
            "193104:  173 games, mean reward -19.160, (epsilon 0.06)\n",
            "Best mean reward updated -19.160\n",
            "194490:  174 games, mean reward -19.150, (epsilon 0.05)\n",
            "Best mean reward updated -19.150\n",
            "195787:  175 games, mean reward -19.140, (epsilon 0.05)\n",
            "Best mean reward updated -19.140\n",
            "197219:  176 games, mean reward -19.110, (epsilon 0.05)\n",
            "Best mean reward updated -19.110\n",
            "199120:  177 games, mean reward -19.050, (epsilon 0.05)\n",
            "Best mean reward updated -19.050\n",
            "200724:  178 games, mean reward -19.000, (epsilon 0.05)\n",
            "Best mean reward updated -19.000\n",
            "202412:  179 games, mean reward -18.950, (epsilon 0.05)\n",
            "Best mean reward updated -18.950\n",
            "204302:  180 games, mean reward -18.880, (epsilon 0.05)\n",
            "Best mean reward updated -18.880\n",
            "205637:  181 games, mean reward -18.880, (epsilon 0.05)\n",
            "207272:  182 games, mean reward -18.840, (epsilon 0.04)\n",
            "Best mean reward updated -18.840\n",
            "208601:  183 games, mean reward -18.820, (epsilon 0.04)\n",
            "Best mean reward updated -18.820\n",
            "210095:  184 games, mean reward -18.810, (epsilon 0.04)\n",
            "Best mean reward updated -18.810\n",
            "211654:  185 games, mean reward -18.760, (epsilon 0.04)\n",
            "Best mean reward updated -18.760\n",
            "213008:  186 games, mean reward -18.770, (epsilon 0.04)\n",
            "214107:  187 games, mean reward -18.750, (epsilon 0.04)\n",
            "Best mean reward updated -18.750\n",
            "215531:  188 games, mean reward -18.750, (epsilon 0.04)\n",
            "217098:  189 games, mean reward -18.720, (epsilon 0.04)\n",
            "Best mean reward updated -18.720\n",
            "218314:  190 games, mean reward -18.700, (epsilon 0.04)\n",
            "Best mean reward updated -18.700\n",
            "219646:  191 games, mean reward -18.680, (epsilon 0.04)\n",
            "Best mean reward updated -18.680\n",
            "220725:  192 games, mean reward -18.690, (epsilon 0.04)\n",
            "222134:  193 games, mean reward -18.670, (epsilon 0.04)\n",
            "Best mean reward updated -18.670\n",
            "223621:  194 games, mean reward -18.650, (epsilon 0.03)\n",
            "Best mean reward updated -18.650\n",
            "225178:  195 games, mean reward -18.630, (epsilon 0.03)\n",
            "Best mean reward updated -18.630\n",
            "227070:  196 games, mean reward -18.550, (epsilon 0.03)\n",
            "Best mean reward updated -18.550\n",
            "228419:  197 games, mean reward -18.550, (epsilon 0.03)\n",
            "230008:  198 games, mean reward -18.520, (epsilon 0.03)\n",
            "Best mean reward updated -18.520\n",
            "231636:  199 games, mean reward -18.510, (epsilon 0.03)\n",
            "Best mean reward updated -18.510\n",
            "233245:  200 games, mean reward -18.470, (epsilon 0.03)\n",
            "Best mean reward updated -18.470\n",
            "234772:  201 games, mean reward -18.440, (epsilon 0.03)\n",
            "Best mean reward updated -18.440\n",
            "236668:  202 games, mean reward -18.390, (epsilon 0.03)\n",
            "Best mean reward updated -18.390\n",
            "238074:  203 games, mean reward -18.360, (epsilon 0.03)\n",
            "Best mean reward updated -18.360\n",
            "240263:  204 games, mean reward -18.300, (epsilon 0.03)\n",
            "Best mean reward updated -18.300\n",
            "241935:  205 games, mean reward -18.270, (epsilon 0.03)\n",
            "Best mean reward updated -18.270\n",
            "244137:  206 games, mean reward -18.180, (epsilon 0.03)\n",
            "Best mean reward updated -18.180\n",
            "245747:  207 games, mean reward -18.180, (epsilon 0.03)\n",
            "247486:  208 games, mean reward -18.150, (epsilon 0.02)\n",
            "Best mean reward updated -18.150\n",
            "249299:  209 games, mean reward -18.100, (epsilon 0.02)\n",
            "Best mean reward updated -18.100\n",
            "251147:  210 games, mean reward -18.010, (epsilon 0.02)\n",
            "Best mean reward updated -18.010\n",
            "252896:  211 games, mean reward -17.970, (epsilon 0.02)\n",
            "Best mean reward updated -17.970\n",
            "254863:  212 games, mean reward -17.890, (epsilon 0.02)\n",
            "Best mean reward updated -17.890\n",
            "256797:  213 games, mean reward -17.820, (epsilon 0.02)\n",
            "Best mean reward updated -17.820\n",
            "258186:  214 games, mean reward -17.790, (epsilon 0.02)\n",
            "Best mean reward updated -17.790\n",
            "259974:  215 games, mean reward -17.710, (epsilon 0.02)\n",
            "Best mean reward updated -17.710\n",
            "261991:  216 games, mean reward -17.620, (epsilon 0.02)\n",
            "Best mean reward updated -17.620\n",
            "263842:  217 games, mean reward -17.540, (epsilon 0.02)\n",
            "Best mean reward updated -17.540\n",
            "265735:  218 games, mean reward -17.480, (epsilon 0.02)\n",
            "Best mean reward updated -17.480\n",
            "267672:  219 games, mean reward -17.400, (epsilon 0.02)\n",
            "Best mean reward updated -17.400\n",
            "269305:  220 games, mean reward -17.380, (epsilon 0.02)\n",
            "Best mean reward updated -17.380\n",
            "271578:  221 games, mean reward -17.310, (epsilon 0.02)\n",
            "Best mean reward updated -17.310\n",
            "273318:  222 games, mean reward -17.280, (epsilon 0.02)\n",
            "Best mean reward updated -17.280\n",
            "275703:  223 games, mean reward -17.200, (epsilon 0.02)\n",
            "Best mean reward updated -17.200\n",
            "278015:  224 games, mean reward -17.090, (epsilon 0.02)\n",
            "Best mean reward updated -17.090\n",
            "279931:  225 games, mean reward -17.010, (epsilon 0.02)\n",
            "Best mean reward updated -17.010\n",
            "281373:  226 games, mean reward -17.030, (epsilon 0.02)\n",
            "283190:  227 games, mean reward -16.950, (epsilon 0.02)\n",
            "Best mean reward updated -16.950\n",
            "285370:  228 games, mean reward -16.840, (epsilon 0.02)\n",
            "Best mean reward updated -16.840\n",
            "287057:  229 games, mean reward -16.810, (epsilon 0.02)\n",
            "Best mean reward updated -16.810\n",
            "288766:  230 games, mean reward -16.790, (epsilon 0.02)\n",
            "Best mean reward updated -16.790\n",
            "290209:  231 games, mean reward -16.730, (epsilon 0.02)\n",
            "Best mean reward updated -16.730\n",
            "291879:  232 games, mean reward -16.710, (epsilon 0.02)\n",
            "Best mean reward updated -16.710\n",
            "293350:  233 games, mean reward -16.680, (epsilon 0.02)\n",
            "Best mean reward updated -16.680\n",
            "295811:  234 games, mean reward -16.620, (epsilon 0.02)\n",
            "Best mean reward updated -16.620\n",
            "297384:  235 games, mean reward -16.610, (epsilon 0.02)\n",
            "Best mean reward updated -16.610\n",
            "299453:  236 games, mean reward -16.550, (epsilon 0.02)\n",
            "Best mean reward updated -16.550\n",
            "300919:  237 games, mean reward -16.520, (epsilon 0.02)\n",
            "Best mean reward updated -16.520\n",
            "303135:  238 games, mean reward -16.430, (epsilon 0.02)\n",
            "Best mean reward updated -16.430\n",
            "305102:  239 games, mean reward -16.380, (epsilon 0.02)\n",
            "Best mean reward updated -16.380\n",
            "306831:  240 games, mean reward -16.390, (epsilon 0.02)\n",
            "308862:  241 games, mean reward -16.310, (epsilon 0.02)\n",
            "Best mean reward updated -16.310\n",
            "310385:  242 games, mean reward -16.260, (epsilon 0.02)\n",
            "Best mean reward updated -16.260\n",
            "311888:  243 games, mean reward -16.240, (epsilon 0.02)\n",
            "Best mean reward updated -16.240\n",
            "312945:  244 games, mean reward -16.260, (epsilon 0.02)\n",
            "314179:  245 games, mean reward -16.230, (epsilon 0.02)\n",
            "Best mean reward updated -16.230\n",
            "315830:  246 games, mean reward -16.160, (epsilon 0.02)\n",
            "Best mean reward updated -16.160\n",
            "317429:  247 games, mean reward -16.090, (epsilon 0.02)\n",
            "Best mean reward updated -16.090\n",
            "319318:  248 games, mean reward -16.040, (epsilon 0.02)\n",
            "Best mean reward updated -16.040\n",
            "320827:  249 games, mean reward -16.020, (epsilon 0.02)\n",
            "Best mean reward updated -16.020\n",
            "322826:  250 games, mean reward -15.980, (epsilon 0.02)\n",
            "Best mean reward updated -15.980\n",
            "324721:  251 games, mean reward -15.910, (epsilon 0.02)\n",
            "Best mean reward updated -15.910\n",
            "326415:  252 games, mean reward -15.890, (epsilon 0.02)\n",
            "Best mean reward updated -15.890\n",
            "328625:  253 games, mean reward -15.820, (epsilon 0.02)\n",
            "Best mean reward updated -15.820\n",
            "330230:  254 games, mean reward -15.770, (epsilon 0.02)\n",
            "Best mean reward updated -15.770\n",
            "332686:  255 games, mean reward -15.650, (epsilon 0.02)\n",
            "Best mean reward updated -15.650\n",
            "334847:  256 games, mean reward -15.580, (epsilon 0.02)\n",
            "Best mean reward updated -15.580\n",
            "336765:  257 games, mean reward -15.510, (epsilon 0.02)\n",
            "Best mean reward updated -15.510\n",
            "338805:  258 games, mean reward -15.460, (epsilon 0.02)\n",
            "Best mean reward updated -15.460\n",
            "340404:  259 games, mean reward -15.420, (epsilon 0.02)\n",
            "Best mean reward updated -15.420\n",
            "342329:  260 games, mean reward -15.350, (epsilon 0.02)\n",
            "Best mean reward updated -15.350\n",
            "343681:  261 games, mean reward -15.330, (epsilon 0.02)\n",
            "Best mean reward updated -15.330\n",
            "345382:  262 games, mean reward -15.290, (epsilon 0.02)\n",
            "Best mean reward updated -15.290\n",
            "348055:  263 games, mean reward -15.200, (epsilon 0.02)\n",
            "Best mean reward updated -15.200\n",
            "349647:  264 games, mean reward -15.170, (epsilon 0.02)\n",
            "Best mean reward updated -15.170\n",
            "351573:  265 games, mean reward -15.100, (epsilon 0.02)\n",
            "Best mean reward updated -15.100\n",
            "353672:  266 games, mean reward -15.020, (epsilon 0.02)\n",
            "Best mean reward updated -15.020\n",
            "355200:  267 games, mean reward -14.990, (epsilon 0.02)\n",
            "Best mean reward updated -14.990\n",
            "357034:  268 games, mean reward -14.970, (epsilon 0.02)\n",
            "Best mean reward updated -14.970\n",
            "359351:  269 games, mean reward -14.870, (epsilon 0.02)\n",
            "Best mean reward updated -14.870\n",
            "361265:  270 games, mean reward -14.840, (epsilon 0.02)\n",
            "Best mean reward updated -14.840\n",
            "363271:  271 games, mean reward -14.810, (epsilon 0.02)\n",
            "Best mean reward updated -14.810\n",
            "365788:  272 games, mean reward -14.740, (epsilon 0.02)\n",
            "Best mean reward updated -14.740\n",
            "367440:  273 games, mean reward -14.720, (epsilon 0.02)\n",
            "Best mean reward updated -14.720\n",
            "369550:  274 games, mean reward -14.640, (epsilon 0.02)\n",
            "Best mean reward updated -14.640\n",
            "371487:  275 games, mean reward -14.580, (epsilon 0.02)\n",
            "Best mean reward updated -14.580\n",
            "373143:  276 games, mean reward -14.550, (epsilon 0.02)\n",
            "Best mean reward updated -14.550\n",
            "375385:  277 games, mean reward -14.520, (epsilon 0.02)\n",
            "Best mean reward updated -14.520\n",
            "378181:  278 games, mean reward -14.420, (epsilon 0.02)\n",
            "Best mean reward updated -14.420\n",
            "380402:  279 games, mean reward -14.390, (epsilon 0.02)\n",
            "Best mean reward updated -14.390\n",
            "382659:  280 games, mean reward -14.360, (epsilon 0.02)\n",
            "Best mean reward updated -14.360\n",
            "384531:  281 games, mean reward -14.330, (epsilon 0.02)\n",
            "Best mean reward updated -14.330\n",
            "386268:  282 games, mean reward -14.310, (epsilon 0.02)\n",
            "Best mean reward updated -14.310\n",
            "388708:  283 games, mean reward -14.220, (epsilon 0.02)\n",
            "Best mean reward updated -14.220\n",
            "390476:  284 games, mean reward -14.160, (epsilon 0.02)\n",
            "Best mean reward updated -14.160\n",
            "392425:  285 games, mean reward -14.150, (epsilon 0.02)\n",
            "Best mean reward updated -14.150\n",
            "394059:  286 games, mean reward -14.120, (epsilon 0.02)\n",
            "Best mean reward updated -14.120\n",
            "395651:  287 games, mean reward -14.080, (epsilon 0.02)\n",
            "Best mean reward updated -14.080\n",
            "397746:  288 games, mean reward -14.000, (epsilon 0.02)\n",
            "Best mean reward updated -14.000\n",
            "399551:  289 games, mean reward -13.980, (epsilon 0.02)\n",
            "Best mean reward updated -13.980\n",
            "401358:  290 games, mean reward -13.920, (epsilon 0.02)\n",
            "Best mean reward updated -13.920\n",
            "403000:  291 games, mean reward -13.870, (epsilon 0.02)\n",
            "Best mean reward updated -13.870\n",
            "404952:  292 games, mean reward -13.760, (epsilon 0.02)\n",
            "Best mean reward updated -13.760\n",
            "406524:  293 games, mean reward -13.720, (epsilon 0.02)\n",
            "Best mean reward updated -13.720\n",
            "408618:  294 games, mean reward -13.630, (epsilon 0.02)\n",
            "Best mean reward updated -13.630\n",
            "410812:  295 games, mean reward -13.550, (epsilon 0.02)\n",
            "Best mean reward updated -13.550\n",
            "412696:  296 games, mean reward -13.550, (epsilon 0.02)\n",
            "414506:  297 games, mean reward -13.470, (epsilon 0.02)\n",
            "Best mean reward updated -13.470\n",
            "416568:  298 games, mean reward -13.410, (epsilon 0.02)\n",
            "Best mean reward updated -13.410\n",
            "418246:  299 games, mean reward -13.380, (epsilon 0.02)\n",
            "Best mean reward updated -13.380\n",
            "419942:  300 games, mean reward -13.370, (epsilon 0.02)\n",
            "Best mean reward updated -13.370\n",
            "421801:  301 games, mean reward -13.330, (epsilon 0.02)\n",
            "Best mean reward updated -13.330\n",
            "423411:  302 games, mean reward -13.320, (epsilon 0.02)\n",
            "Best mean reward updated -13.320\n",
            "425327:  303 games, mean reward -13.290, (epsilon 0.02)\n",
            "Best mean reward updated -13.290\n",
            "427538:  304 games, mean reward -13.260, (epsilon 0.02)\n",
            "Best mean reward updated -13.260\n",
            "430402:  305 games, mean reward -13.180, (epsilon 0.02)\n",
            "Best mean reward updated -13.180\n",
            "433364:  306 games, mean reward -13.090, (epsilon 0.02)\n",
            "Best mean reward updated -13.090\n",
            "435962:  307 games, mean reward -12.970, (epsilon 0.02)\n",
            "Best mean reward updated -12.970\n",
            "438175:  308 games, mean reward -12.900, (epsilon 0.02)\n",
            "Best mean reward updated -12.900\n",
            "440212:  309 games, mean reward -12.890, (epsilon 0.02)\n",
            "Best mean reward updated -12.890\n",
            "442187:  310 games, mean reward -12.880, (epsilon 0.02)\n",
            "Best mean reward updated -12.880\n",
            "443769:  311 games, mean reward -12.880, (epsilon 0.02)\n",
            "445725:  312 games, mean reward -12.870, (epsilon 0.02)\n",
            "Best mean reward updated -12.870\n",
            "448809:  313 games, mean reward -12.720, (epsilon 0.02)\n",
            "Best mean reward updated -12.720\n",
            "450587:  314 games, mean reward -12.710, (epsilon 0.02)\n",
            "Best mean reward updated -12.710\n",
            "452809:  315 games, mean reward -12.690, (epsilon 0.02)\n",
            "Best mean reward updated -12.690\n",
            "454551:  316 games, mean reward -12.700, (epsilon 0.02)\n",
            "456640:  317 games, mean reward -12.690, (epsilon 0.02)\n",
            "459110:  318 games, mean reward -12.640, (epsilon 0.02)\n",
            "Best mean reward updated -12.640\n",
            "461591:  319 games, mean reward -12.610, (epsilon 0.02)\n",
            "Best mean reward updated -12.610\n",
            "463701:  320 games, mean reward -12.550, (epsilon 0.02)\n",
            "Best mean reward updated -12.550\n",
            "465911:  321 games, mean reward -12.520, (epsilon 0.02)\n",
            "Best mean reward updated -12.520\n",
            "468177:  322 games, mean reward -12.460, (epsilon 0.02)\n",
            "Best mean reward updated -12.460\n",
            "470927:  323 games, mean reward -12.390, (epsilon 0.02)\n",
            "Best mean reward updated -12.390\n",
            "472752:  324 games, mean reward -12.410, (epsilon 0.02)\n",
            "475010:  325 games, mean reward -12.380, (epsilon 0.02)\n",
            "Best mean reward updated -12.380\n",
            "477224:  326 games, mean reward -12.300, (epsilon 0.02)\n",
            "Best mean reward updated -12.300\n",
            "479498:  327 games, mean reward -12.270, (epsilon 0.02)\n",
            "Best mean reward updated -12.270\n",
            "481367:  328 games, mean reward -12.320, (epsilon 0.02)\n",
            "483777:  329 games, mean reward -12.270, (epsilon 0.02)\n",
            "486320:  330 games, mean reward -12.190, (epsilon 0.02)\n",
            "Best mean reward updated -12.190\n",
            "488596:  331 games, mean reward -12.150, (epsilon 0.02)\n",
            "Best mean reward updated -12.150\n",
            "491351:  332 games, mean reward -12.030, (epsilon 0.02)\n",
            "Best mean reward updated -12.030\n",
            "493924:  333 games, mean reward -11.910, (epsilon 0.02)\n",
            "Best mean reward updated -11.910\n",
            "495945:  334 games, mean reward -11.910, (epsilon 0.02)\n",
            "497997:  335 games, mean reward -11.850, (epsilon 0.02)\n",
            "Best mean reward updated -11.850\n",
            "500330:  336 games, mean reward -11.840, (epsilon 0.02)\n",
            "Best mean reward updated -11.840\n",
            "502250:  337 games, mean reward -11.800, (epsilon 0.02)\n",
            "Best mean reward updated -11.800\n",
            "504613:  338 games, mean reward -11.800, (epsilon 0.02)\n",
            "506760:  339 games, mean reward -11.780, (epsilon 0.02)\n",
            "Best mean reward updated -11.780\n",
            "508642:  340 games, mean reward -11.750, (epsilon 0.02)\n",
            "Best mean reward updated -11.750\n",
            "510602:  341 games, mean reward -11.770, (epsilon 0.02)\n",
            "512440:  342 games, mean reward -11.790, (epsilon 0.02)\n",
            "515144:  343 games, mean reward -11.710, (epsilon 0.02)\n",
            "Best mean reward updated -11.710\n",
            "517512:  344 games, mean reward -11.620, (epsilon 0.02)\n",
            "Best mean reward updated -11.620\n",
            "519148:  345 games, mean reward -11.590, (epsilon 0.02)\n",
            "Best mean reward updated -11.590\n",
            "520596:  346 games, mean reward -11.620, (epsilon 0.02)\n",
            "522063:  347 games, mean reward -11.650, (epsilon 0.02)\n",
            "523606:  348 games, mean reward -11.670, (epsilon 0.02)\n",
            "525750:  349 games, mean reward -11.620, (epsilon 0.02)\n",
            "528158:  350 games, mean reward -11.610, (epsilon 0.02)\n",
            "530874:  351 games, mean reward -11.570, (epsilon 0.02)\n",
            "Best mean reward updated -11.570\n",
            "533617:  352 games, mean reward -11.450, (epsilon 0.02)\n",
            "Best mean reward updated -11.450\n",
            "536178:  353 games, mean reward -11.400, (epsilon 0.02)\n",
            "Best mean reward updated -11.400\n",
            "538909:  354 games, mean reward -11.320, (epsilon 0.02)\n",
            "Best mean reward updated -11.320\n",
            "541007:  355 games, mean reward -11.360, (epsilon 0.02)\n",
            "542757:  356 games, mean reward -11.410, (epsilon 0.02)\n",
            "545234:  357 games, mean reward -11.390, (epsilon 0.02)\n",
            "547440:  358 games, mean reward -11.400, (epsilon 0.02)\n",
            "549405:  359 games, mean reward -11.370, (epsilon 0.02)\n",
            "552273:  360 games, mean reward -11.330, (epsilon 0.02)\n",
            "554241:  361 games, mean reward -11.290, (epsilon 0.02)\n",
            "Best mean reward updated -11.290\n",
            "556868:  362 games, mean reward -11.210, (epsilon 0.02)\n",
            "Best mean reward updated -11.210\n",
            "558830:  363 games, mean reward -11.260, (epsilon 0.02)\n",
            "561288:  364 games, mean reward -11.170, (epsilon 0.02)\n",
            "Best mean reward updated -11.170\n",
            "563508:  365 games, mean reward -11.180, (epsilon 0.02)\n",
            "565914:  366 games, mean reward -11.150, (epsilon 0.02)\n",
            "Best mean reward updated -11.150\n",
            "568188:  367 games, mean reward -11.080, (epsilon 0.02)\n",
            "Best mean reward updated -11.080\n",
            "570832:  368 games, mean reward -10.970, (epsilon 0.02)\n",
            "Best mean reward updated -10.970\n",
            "574178:  369 games, mean reward -10.870, (epsilon 0.02)\n",
            "Best mean reward updated -10.870\n",
            "576519:  370 games, mean reward -10.800, (epsilon 0.02)\n",
            "Best mean reward updated -10.800\n",
            "578582:  371 games, mean reward -10.810, (epsilon 0.02)\n",
            "581631:  372 games, mean reward -10.770, (epsilon 0.02)\n",
            "Best mean reward updated -10.770\n",
            "584130:  373 games, mean reward -10.690, (epsilon 0.02)\n",
            "Best mean reward updated -10.690\n",
            "586650:  374 games, mean reward -10.660, (epsilon 0.02)\n",
            "Best mean reward updated -10.660\n",
            "588809:  375 games, mean reward -10.670, (epsilon 0.02)\n",
            "591114:  376 games, mean reward -10.610, (epsilon 0.02)\n",
            "Best mean reward updated -10.610\n",
            "593804:  377 games, mean reward -10.570, (epsilon 0.02)\n",
            "Best mean reward updated -10.570\n",
            "597069:  378 games, mean reward -10.550, (epsilon 0.02)\n",
            "Best mean reward updated -10.550\n",
            "599480:  379 games, mean reward -10.520, (epsilon 0.02)\n",
            "Best mean reward updated -10.520\n",
            "601576:  380 games, mean reward -10.520, (epsilon 0.02)\n",
            "604514:  381 games, mean reward -10.400, (epsilon 0.02)\n",
            "Best mean reward updated -10.400\n",
            "607281:  382 games, mean reward -10.320, (epsilon 0.02)\n",
            "Best mean reward updated -10.320\n",
            "609178:  383 games, mean reward -10.380, (epsilon 0.02)\n",
            "611936:  384 games, mean reward -10.280, (epsilon 0.02)\n",
            "Best mean reward updated -10.280\n",
            "614804:  385 games, mean reward -10.170, (epsilon 0.02)\n",
            "Best mean reward updated -10.170\n",
            "617517:  386 games, mean reward -10.070, (epsilon 0.02)\n",
            "Best mean reward updated -10.070\n",
            "619838:  387 games, mean reward -10.020, (epsilon 0.02)\n",
            "Best mean reward updated -10.020\n",
            "622529:  388 games, mean reward -9.870, (epsilon 0.02)\n",
            "Best mean reward updated -9.870\n",
            "624986:  389 games, mean reward -9.810, (epsilon 0.02)\n",
            "Best mean reward updated -9.810\n",
            "628043:  390 games, mean reward -9.690, (epsilon 0.02)\n",
            "Best mean reward updated -9.690\n",
            "630613:  391 games, mean reward -9.610, (epsilon 0.02)\n",
            "Best mean reward updated -9.610\n",
            "632675:  392 games, mean reward -9.630, (epsilon 0.02)\n",
            "635747:  393 games, mean reward -9.520, (epsilon 0.02)\n",
            "Best mean reward updated -9.520\n",
            "638443:  394 games, mean reward -9.400, (epsilon 0.02)\n",
            "Best mean reward updated -9.400\n",
            "641295:  395 games, mean reward -9.280, (epsilon 0.02)\n",
            "Best mean reward updated -9.280\n",
            "644131:  396 games, mean reward -9.200, (epsilon 0.02)\n",
            "Best mean reward updated -9.200\n",
            "646962:  397 games, mean reward -9.110, (epsilon 0.02)\n",
            "Best mean reward updated -9.110\n",
            "649766:  398 games, mean reward -9.030, (epsilon 0.02)\n",
            "Best mean reward updated -9.030\n",
            "652699:  399 games, mean reward -8.920, (epsilon 0.02)\n",
            "Best mean reward updated -8.920\n",
            "655716:  400 games, mean reward -8.790, (epsilon 0.02)\n",
            "Best mean reward updated -8.790\n",
            "658698:  401 games, mean reward -8.700, (epsilon 0.02)\n",
            "Best mean reward updated -8.700\n",
            "661658:  402 games, mean reward -8.600, (epsilon 0.02)\n",
            "Best mean reward updated -8.600\n",
            "664885:  403 games, mean reward -8.480, (epsilon 0.02)\n",
            "Best mean reward updated -8.480\n",
            "667787:  404 games, mean reward -8.350, (epsilon 0.02)\n",
            "Best mean reward updated -8.350\n",
            "670773:  405 games, mean reward -8.300, (epsilon 0.02)\n",
            "Best mean reward updated -8.300\n",
            "672750:  406 games, mean reward -8.390, (epsilon 0.02)\n",
            "675468:  407 games, mean reward -8.380, (epsilon 0.02)\n",
            "678325:  408 games, mean reward -8.310, (epsilon 0.02)\n",
            "680742:  409 games, mean reward -8.260, (epsilon 0.02)\n",
            "Best mean reward updated -8.260\n",
            "683903:  410 games, mean reward -8.140, (epsilon 0.02)\n",
            "Best mean reward updated -8.140\n",
            "687116:  411 games, mean reward -8.000, (epsilon 0.02)\n",
            "Best mean reward updated -8.000\n",
            "690184:  412 games, mean reward -7.900, (epsilon 0.02)\n",
            "Best mean reward updated -7.900\n",
            "692672:  413 games, mean reward -7.990, (epsilon 0.02)\n",
            "695536:  414 games, mean reward -7.880, (epsilon 0.02)\n",
            "Best mean reward updated -7.880\n",
            "698409:  415 games, mean reward -7.800, (epsilon 0.02)\n",
            "Best mean reward updated -7.800\n",
            "701372:  416 games, mean reward -7.730, (epsilon 0.02)\n",
            "Best mean reward updated -7.730\n",
            "704414:  417 games, mean reward -7.640, (epsilon 0.02)\n",
            "Best mean reward updated -7.640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 2"
      ],
      "metadata": {
        "id": "hwD9YJQ4Vov2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "dfchN9kxIdcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rz53ZdhIt-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "metadata": {
        "id": "rvrJxFSaJVlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmdDmgqpJApU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "############################################################################################"
      ],
      "metadata": {
        "id": "Hgzu3mcUTuXC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6Q0XuFu-LaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gELty18e-QZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = gym.make(ENVIRONMENT)  # Get env\n",
        "agent = Agent(environment)  # Create Agent\n",
        "\n",
        "if LOAD_MODEL_FROM_FILE:\n",
        "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
        "\n",
        "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
        "        param = json.load(outfile)\n",
        "        agent.epsilon = param.get('epsilon')\n",
        "\n",
        "    startEpisode = LOAD_FILE_EPISODE + 1\n",
        "\n",
        "else:\n",
        "    startEpisode = 1\n",
        "\n",
        "last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
        "total_step = 1  # Cumulkative sum of all steps in episodes\n",
        "for episode in range(startEpisode, MAX_EPISODE):\n",
        "\n",
        "    startTime = time.time()  # Keep time\n",
        "    state = environment.reset()  # Reset env\n",
        "\n",
        "    state = agent.preProcess(state)  # Process image\n",
        "\n",
        "    # Stack state . Every state contains 4 time contionusly frames\n",
        "    # We stack frames like 4 channel image\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    total_max_q_val = 0  # Total max q vals\n",
        "    total_reward = 0  # Total reward for each episode\n",
        "    total_loss = 0  # Total loss for each episode\n",
        "    for step in range(MAX_STEP):\n",
        "\n",
        "        if RENDER_GAME_WINDOW:\n",
        "            environment.render()  # Show state visually\n",
        "\n",
        "        # Select and perform an action\n",
        "        action = agent.act(state)  # Act\n",
        "        next_state, reward, done, info = environment.step(action)  # Observe\n",
        "\n",
        "        next_state = agent.preProcess(next_state)  # Process image\n",
        "\n",
        "        # Stack state . Every state contains 4 time contionusly frames\n",
        "        # We stack frames like 4 channel image\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state  # Update state\n",
        "\n",
        "        if TRAIN_MODEL:\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
        "        else:\n",
        "            loss, max_q_val = [0, 0]\n",
        "\n",
        "        total_loss += loss\n",
        "        total_max_q_val += max_q_val\n",
        "        total_reward += reward\n",
        "        total_step += 1\n",
        "        if total_step % 1000 == 0:\n",
        "            agent.adaptiveEpsilon()  # Decrase epsilon\n",
        "\n",
        "        if done:  # Episode completed\n",
        "            currentTime = time.time()  # Keep current time\n",
        "            time_passed = currentTime - startTime  # Find episode duration\n",
        "            current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
        "            epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
        "\n",
        "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
        "                weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
        "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
        "\n",
        "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                with open(epsilonPath, 'w') as outfile:\n",
        "                    json.dump(epsilonDict, outfile)\n",
        "\n",
        "            if TRAIN_MODEL:\n",
        "                agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
        "\n",
        "            last_100_ep_reward.append(total_reward)\n",
        "            avg_max_q_val = total_max_q_val / step\n",
        "\n",
        "            outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
        "                episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
        "            )\n",
        "\n",
        "            print(outStr)\n",
        "\n",
        "            if SAVE_MODELS:\n",
        "                outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
        "                with open(outputPath, 'a') as outfile:\n",
        "                    outfile.write(outStr+\"\\n\")\n",
        "\n",
        "            break"
      ],
      "metadata": {
        "id": "J2CJ5YgG-VGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "id": "3nyufv3E-Yb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(environment, policy, optimizer, discount_factor, something): # Placeholder for your train function\n",
        "    # Calculate loss and train_reward here\n",
        "    # This is just a placeholder, replace this with your actual implementation\n",
        "    loss = 0\n",
        "    train_reward = 0\n",
        "    return loss, train_reward"
      ],
      "metadata": {
        "id": "JBybY3ADNDSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def objective(trial):\n",
        "    # Calculate input dimension based on environment and preprocessing\n",
        "    environment = gym.make(ENVIRONMENT)  # Get env\n",
        "    state_size_h = environment.observation_space.shape[0]\n",
        "    state_size_w = environment.observation_space.shape[1]\n",
        "    target_h = 80  # Height after process\n",
        "    target_w = 64  # Widht after process\n",
        "    INPUT_DIM = int((target_h / 4) * (target_w / 4) * 64)  # Calculate input dimension\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    OUTPUT_DIM = environment.action_space.n  # Output dimension based on action space\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 256, 1024)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "\n",
        "    # Initialize model, optimizer with suggested hyperparameters\n",
        "    policy = SNN(target_h, target_w, OUTPUT_DIM, dropout).to(device) # Use target_h and target_w here\n",
        "    policy.apply(init_weights)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop (simplified for brevity)\n",
        "    MAX_EPISODES = 100\n",
        "    DISCOUNT_FACTOR = 0.99\n",
        "    for episode in range(1, MAX_EPISODES + 1):\n",
        "        loss, train_reward = train(environment, policy, optimizer, DISCOUNT_FACTOR, []) # Use environment here\n",
        "\n",
        "        # Evaluate on validation set - Replace this with your actual validation logic\n",
        "        #validation_reward = evaluate(environment, policy) # Example: Assuming you have a validation_env\n",
        "        validation_reward = 0\n",
        "        trial.report(validation_reward, episode)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return validation_reward"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dHc3tGCmMODI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define init_weights function\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "ms7zdqbRMuyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")  # Maximize validation reward\n",
        "study.optimize(objective, n_trials=500)  # Run 100 trials"
      ],
      "metadata": {
        "id": "Zvg8XWD2KVpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}